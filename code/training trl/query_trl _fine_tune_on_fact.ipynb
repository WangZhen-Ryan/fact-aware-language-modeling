{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f8d3b6f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import random\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "import nltk\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "import bs4\n",
    "import requests\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "from spacy.matcher import Matcher \n",
    "from spacy.tokens import Span \n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c675c9cf",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# import relevant LMs package\n",
    "\n",
    "import transformers\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "from transformers import BertForSequenceClassification, AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from transformers import GPT2Model, GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05a5cd88",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use gpu\n"
     ]
    }
   ],
   "source": [
    "# cheak to use gpu or cpu\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Use gpu\")\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    print(\"Use cpu\")\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1f2307f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "device = \"cuda:0\"\n",
    "torch.cuda.empty_cache()\n",
    "# torch.cuda.reset_max_memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "873d527b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>query</th>\n",
       "      <th>response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8015189</td>\n",
       "      <td>WCSV city</td>\n",
       "      <td>Crossville, Tennessee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5808923</td>\n",
       "      <td>Prototype (video game) computingPlatform</td>\n",
       "      <td>PlayStation 3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3446381</td>\n",
       "      <td>Nick Brockmeyer occupation</td>\n",
       "      <td>Nick Brockmeyer  1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10543677</td>\n",
       "      <td>Arbitrary Execution musicalBand</td>\n",
       "      <td>X Marks the Pedwalk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3961308</td>\n",
       "      <td>Ferreira do Z%C3%AAzere Municipality ://xmlns.com/foaf/0.1/homepage</td>\n",
       "      <td>p://www.cm-ferreiradozezere.pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79995</th>\n",
       "      <td>6001114</td>\n",
       "      <td>We%C5%9Brednik isPartOf</td>\n",
       "      <td>Chodzie%C5%BC County</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79996</th>\n",
       "      <td>3672215</td>\n",
       "      <td>Imaculado Cora%C3%A7%C3%A3o de Maria type</td>\n",
       "      <td>Parish (administrative division)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79997</th>\n",
       "      <td>10702379</td>\n",
       "      <td>B%C4%83ne%C5%9Fti River (H%C4%83lm%C4%83gel) mouthPlace</td>\n",
       "      <td>H%C4%83lmagiu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79998</th>\n",
       "      <td>6659309</td>\n",
       "      <td>Ghosts and Vodka bandMember</td>\n",
       "      <td>Erik Bocek</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79999</th>\n",
       "      <td>6155646</td>\n",
       "      <td>Penonom%C3%A9, Cocl%C3%A9 isPartOf</td>\n",
       "      <td>Cocl%C3%A9 Province</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>80000 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0  \\\n",
       "0         8015189   \n",
       "1         5808923   \n",
       "2         3446381   \n",
       "3        10543677   \n",
       "4         3961308   \n",
       "...           ...   \n",
       "79995     6001114   \n",
       "79996     3672215   \n",
       "79997    10702379   \n",
       "79998     6659309   \n",
       "79999     6155646   \n",
       "\n",
       "                                                                     query  \\\n",
       "0                                                                WCSV city   \n",
       "1                                 Prototype (video game) computingPlatform   \n",
       "2                                               Nick Brockmeyer occupation   \n",
       "3                                          Arbitrary Execution musicalBand   \n",
       "4      Ferreira do Z%C3%AAzere Municipality ://xmlns.com/foaf/0.1/homepage   \n",
       "...                                                                    ...   \n",
       "79995                                              We%C5%9Brednik isPartOf   \n",
       "79996                            Imaculado Cora%C3%A7%C3%A3o de Maria type   \n",
       "79997              B%C4%83ne%C5%9Fti River (H%C4%83lm%C4%83gel) mouthPlace   \n",
       "79998                                          Ghosts and Vodka bandMember   \n",
       "79999                                   Penonom%C3%A9, Cocl%C3%A9 isPartOf   \n",
       "\n",
       "                               response  \n",
       "0                 Crossville, Tennessee  \n",
       "1                         PlayStation 3  \n",
       "2                    Nick Brockmeyer  1  \n",
       "3                   X Marks the Pedwalk  \n",
       "4        p://www.cm-ferreiradozezere.pt  \n",
       "...                                 ...  \n",
       "79995              Chodzie%C5%BC County  \n",
       "79996  Parish (administrative division)  \n",
       "79997                     H%C4%83lmagiu  \n",
       "79998                        Erik Bocek  \n",
       "79999               Cocl%C3%A9 Province  \n",
       "\n",
       "[80000 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path='C:\\\\Users\\\\wz\\\\Desktop\\\\context-aware-embedding-master\\\\Resources\\\\code\\\\data\\\\DBpedia_query\\\\train\\\\DBpedia_query_train.csv'\n",
    "# path='C:\\\\Users\\\\wz\\\\Desktop\\\\DBpedia_query\\\\train\\\\DBpedia_query_train.csv'\n",
    "DBpedia = pd.read_csv(path)\n",
    "DBpedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2753cef5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Begin next-word using HF GPT-2 demo \n"
     ]
    }
   ],
   "source": [
    "# next token prediction\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "\n",
    "print(\"\\nBegin next-word using HF GPT-2 demo \")\n",
    "\n",
    "toker = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc2c2983",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# from  transformers import GPT2TokenizerFast, GPT2LMHeadModel\n",
    "\n",
    "# base_tokenizer = GPT2TokenizerFast.from_pretrained('gpt2')\n",
    "# vocabulary = base_tokenizer.get_vocab()\n",
    "# vocabulary['University']\n",
    "\n",
    "# text = 'University of Wisconsin%E2%80%93Madison'\n",
    "# sub = 'University'\n",
    "# text_ids = base_tokenizer.encode(sub, return_tensors = 'pt')\n",
    "# text_ids.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "284026a9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "# from  transformers import GPT2TokenizerFast, GPT2LMHeadModel\n",
    "# from torch.nn import CrossEntropyLoss\n",
    "\n",
    "# model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "# model.eval()\n",
    "# tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "\n",
    "# ### We calculate the hidden_states and the past of the common left part of the sentence\n",
    "# past = \"I like sitting in my new chair and\"\n",
    "# past_tokenize_input = tokenizer.tokenize(past)\n",
    "# past_tensor_input = torch.tensor([tokenizer.convert_tokens_to_ids(past_tokenize_input)])\n",
    "\n",
    "# past_last_hidden_state, past = model.transformer(past_tensor_input)\n",
    "\n",
    "# print(past_last_hidden_state)\n",
    "\n",
    "# def score(sentence, past, past_last_hidden_state, past_tensor_input):\n",
    "#     tokenize_input = tokenizer.tokenize(sentence, )\n",
    "#     tensor_input = torch.tensor([tokenizer.convert_tokens_to_ids(tokenize_input)])\n",
    "\n",
    "#     ###the following code is slightly modified from https://github.com/huggingface/transformers/blob/09a2f40684f77e62d0fd8485fe9d2d610390453f/src/transformers/modeling_gpt2.py#L604\n",
    "#     ###now we calculate the right part of the sentence with the already calculated past\n",
    "#     transformer_outputs = model.transformer(\n",
    "#             tensor_input,\n",
    "#             attention_mask=None,\n",
    "#             token_type_ids=None,\n",
    "#             position_ids=None,\n",
    "#             head_mask=None,\n",
    "#             inputs_embeds=None,\n",
    "#             use_cache=None,\n",
    "#             output_attentions=None,\n",
    "#             output_hidden_states=None,\n",
    "#         )\n",
    "    \n",
    "#     # print(past_last_hidden_state)\n",
    "        \n",
    "#     ###and concatenate the output of with the hidden_state of the left part of the sentence\n",
    "#     hidden_states = torch.cat((past_last_hidden_state, transformer_outputs[0]), dim=1)\n",
    "    \n",
    "#     ###the following part is exactly the same as https://github.com/huggingface/transformers/blob/09a2f40684f77e62d0fd8485fe9d2d610390453f/src/transformers/modeling_gpt2.py#L604\n",
    "#     lm_logits = model.lm_head(hidden_states)\n",
    "\n",
    "#     labels_input = torch.cat((past_tensor_input, tensor_input), dim=1)\n",
    "\n",
    "#     # Shift so that tokens < n predict n\n",
    "#     shift_logits = lm_logits[..., :-1, :].contiguous()\n",
    "#     shift_labels = labels_input[..., 1:].contiguous()\n",
    "#     # Flatten the tokens\n",
    "#     loss_fct = CrossEntropyLoss()\n",
    "#     loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "#     return -loss.item()\n",
    "\n",
    "# candidates = [\"watch\", \"run\", \"think\", \"apple\", \"light\"]\n",
    "\n",
    "# sent_template = \" {} about life\"\n",
    "\n",
    "# print({candidate: score((sent_template.format(candidate)), past, past_last_hidden_state, past_tensor_input) for candidate in candidates})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f3e6b79",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# get a subgraph that include the true answer with input row\n",
    "def get_label(row, n):\n",
    "    text = DBpedia.loc[3,\"query\"]\n",
    "    true_answer = DBpedia.loc[3,\"response\"]\n",
    "    DBpedia_s = DBpedia.sample(n-1)\n",
    "    DBpedia_row = DBpedia.iloc[[row]]\n",
    "    DBpedia_sub = pd.concat([DBpedia_s,DBpedia_row], axis=0)\n",
    "    # get labels\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    all_response = DBpedia_sub.loc[:,'response'].values\n",
    "    le.fit(all_response)\n",
    "    label_value = le.transform(all_response)\n",
    "    # form a dict labels\n",
    "    labels = dict()\n",
    "    for i in range(len(all_response)):\n",
    "        labels[all_response[i]] = label_value[i]\n",
    "    # print(len(all_response))\n",
    "    return text, true_answer, all_response, label_value, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea7da641",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# all_response = all_response[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "21040685",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "model.eval()\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "def next_word_prediction(sentence):\n",
    "    tokenize_input = tokenizer.tokenize(sentence)\n",
    "    tensor_input = torch.tensor([tokenizer.convert_tokens_to_ids(tokenize_input)])\n",
    "    loss = model(tensor_input, labels=tensor_input)\n",
    "    return -loss[0].item()\n",
    "\n",
    "def outcome(candidates, sentence, true_answer):\n",
    "    # candidates = all_response\n",
    "    sent_template = sentence + \" {}\"\n",
    "    # print({candidate: next_word_prediction(sent_template.format(candidate)) for candidate in candidates})\n",
    "    response_prob = {candidate: next_word_prediction(sent_template.format(candidate)) for candidate in candidates}\n",
    "    max_key = max(response_prob, key=lambda k: response_prob[k])\n",
    "    # loss from true response \n",
    "    loss = response_prob[max_key] - response_prob[true_answer]\n",
    "    return max_key, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0507b977",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def next_word_GPT2(row, n):\n",
    "    # print( get_label(3,100))\n",
    "    text, true_answer, all_response, label_value, labels = get_label(3,100)\n",
    "    max_key, loss = outcome(all_response,text,true_answer)\n",
    "    return max_key, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8cdb3a05",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Agremia%C3%A7%C3%A3o Sportiva Arapiraquense', 3.9958648681640625)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_word_GPT2(10,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3040a60d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# # get a bound for loss for 3 trail\n",
    "# for it in range(3):\n",
    "#     array = []\n",
    "#     for i in tqdm(range(len(DBpedia))):\n",
    "#         array.append(next_word_GPT2(i,100))\n",
    "#     print(\"The median is \", np.median(array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f9961f89",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# np.median(array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "67adfbaf",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import wandb\n",
    "import time\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from random import choices\n",
    "import matplotlib.pyplot as plt\n",
    "tqdm.pandas()\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import GPT2Tokenizer\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "from trl.gpt2 import GPT2HeadWithValueModel, respond_to_batch\n",
    "from trl.ppo import PPOTrainer\n",
    "from trl.core import build_bert_batch_from_txt\n",
    "\n",
    "config = {\n",
    "    \"lm_name\": \"lvwerra/gpt2\",\n",
    "    \"ref_lm_name\": \"lvwerra/gpt2\",\n",
    "    \"tk_name\": \"gpt2\",\n",
    "    \"steps\": 6400,\n",
    "    \"batch_size\": 64,\n",
    "    \"forward_batch_size\": 8,\n",
    "    \"ppo_epochs\": 4,   \n",
    "    \"txt_in_len\": 5,\n",
    "    \"txt_out_len\": 15,\n",
    "    \"lr\": 1.41e-5,\n",
    "    \"init_kl_coef\":0.2,\n",
    "    \"target\": 6,\n",
    "    \"horizon\":10000,\n",
    "    \"gamma\":1,\n",
    "    \"lam\":0.95,\n",
    "    \"cliprange\": .2,\n",
    "    \"cliprange_value\":.2,\n",
    "    \"vf_coef\":.1, \n",
    "    \"seed\": 1,\n",
    "}\n",
    "\n",
    "np.random.seed(config['seed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "420ce385",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mwz_ryan\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\wz\\Desktop\\context-aware-embedding-master\\Resources\\code\\wandb\\run-20221031_051042-2h8zagx2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/wz_ryan/gpt2-query/runs/2h8zagx2\" target=\"_blank\">trails #11</a></strong> to <a href=\"https://wandb.ai/wz_ryan/gpt2-query\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/wz_ryan/gpt2-query/runs/2h8zagx2?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x15b9d2985b0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trails\n",
    "wandb.init(name='trails #11', project='gpt2-query', config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5b8d18c6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2HeadWithValueModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.0.attn.masked_bias', 'h.1.attn.masked_bias', 'h.2.attn.masked_bias', 'h.3.attn.masked_bias', 'h.4.attn.masked_bias', 'h.5.attn.masked_bias', 'h.6.attn.masked_bias', 'h.7.attn.masked_bias', 'h.8.attn.masked_bias', 'h.9.attn.masked_bias', 'h.10.attn.masked_bias', 'h.11.attn.masked_bias', 'lm_head.weight', 'v_head.summary.weight', 'v_head.summary.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of GPT2HeadWithValueModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.0.attn.masked_bias', 'h.1.attn.masked_bias', 'h.2.attn.masked_bias', 'h.3.attn.masked_bias', 'h.4.attn.masked_bias', 'h.5.attn.masked_bias', 'h.6.attn.masked_bias', 'h.7.attn.masked_bias', 'h.8.attn.masked_bias', 'h.9.attn.masked_bias', 'h.10.attn.masked_bias', 'h.11.attn.masked_bias', 'lm_head.weight', 'v_head.summary.weight', 'v_head.summary.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# load models\n",
    "gpt2_model = GPT2HeadWithValueModel.from_pretrained(\"gpt2\")\n",
    "gpt2_model_ref = GPT2HeadWithValueModel.from_pretrained(\"gpt2\")\n",
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained(config['tk_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5159c521",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "_ = gpt2_model.to(device)\n",
    "_ = gpt2_model_ref.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "61492b18",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.watch(gpt2_model, log='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3699f42f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# bert classifier\n",
    "# unused\n",
    "device = \"cuda:0\"\n",
    "sentiment_model = AutoModelForSequenceClassification.from_pretrained(\"lvwerra/distilbert-imdb\")\n",
    "sentiment_tokenizer = AutoTokenizer.from_pretrained(\"lvwerra/distilbert-imdb\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0c7e5c12",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# rename\n",
    "df = DBpedia.copy()\n",
    "ctrl_str = ['[false]', '[true]']\n",
    "ctrl_tokens = dict((s, gpt2_tokenizer.encode(s, return_tensors=\"pt\").squeeze().to(device)) for s in ctrl_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "67929ef7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# special token\n",
    "df['tokens'] = df['query'].apply(lambda x: gpt2_tokenizer.encode(' '+str(x), return_tensors=\"pt\").to(device))\n",
    "df['tokens'] = df['tokens'].apply(lambda x: x[0])\n",
    "df['tokens'] = df['tokens'].apply(lambda x: x[:2])\n",
    "\n",
    "# dropping invalid length in token column\n",
    "drop_row_index = []\n",
    "for i in range(1000):\n",
    "\n",
    "    if len(df['tokens'].loc[i]) < 2:\n",
    "        #df['tokens'].loc[i] = df['tokens'].loc[0].cuda().detach().cpu().clone().numpy()\n",
    "        drop_row_index.append(i)\n",
    "print(drop_row_index)\n",
    "df = df.drop(df.index[drop_row_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcaa9aa1",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### load trained model from finetune on facts 0/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "10a99dba",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class SimpleGPT2SequenceClassifier(nn.Module):\n",
    "    def __init__(self, hidden_size: int, num_classes:int ,max_seq_len:int, gpt_model_name:str):\n",
    "        super(SimpleGPT2SequenceClassifier,self).__init__()\n",
    "        self.gpt2model = GPT2Model.from_pretrained(gpt_model_name)\n",
    "        self.fc1 = nn.Linear(hidden_size*max_seq_len, num_classes)\n",
    "\n",
    "        \n",
    "    def forward(self, input_id, mask):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "                input_id: encoded inputs ids of sent.\n",
    "        \"\"\"\n",
    "        gpt_out, _ = self.gpt2model(input_ids=input_id, attention_mask=mask, return_dict=False)\n",
    "        batch_size = gpt_out.shape[0]\n",
    "        linear_output = self.fc1(gpt_out.view(batch_size,-1))\n",
    "        return linear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "17ce02c3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2Model were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.0.attn.masked_bias', 'h.1.attn.masked_bias', 'h.2.attn.masked_bias', 'h.3.attn.masked_bias', 'h.4.attn.masked_bias', 'h.5.attn.masked_bias', 'h.6.attn.masked_bias', 'h.7.attn.masked_bias', 'h.8.attn.masked_bias', 'h.9.attn.masked_bias', 'h.10.attn.masked_bias', 'h.11.attn.masked_bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SimpleGPT2SequenceClassifier(\n",
       "  (gpt2model): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (fc1): Linear(in_features=7680, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_new = SimpleGPT2SequenceClassifier(hidden_size=768, num_classes=2, max_seq_len=10, gpt_model_name=\"gpt2\")\n",
    "model_new.load_state_dict(torch.load(\"C:\\\\Users\\\\wz\\\\Desktop\\\\context-aware-embedding-master\\\\Resources\\\\code\\\\\\Finetune on 0,1\\\\model\\\\gpt2-text-classifier-model.pt\"))\n",
    "model_new.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e0ad84",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### use the model we trained on fact "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1c6e6200",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "example_text = \"\"\"\n",
    "Fairview, California country Iran\n",
    "\"\"\"\n",
    "def fact_eval(text):\n",
    "    fixed_text = \" \".join(example_text.lower().split())\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    tokenizer.padding_side = \"left\"\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    model_input = tokenizer(fixed_text, padding='max_length', max_length=10, truncation=True, return_tensors=\"pt\")\n",
    "    mask = model_input['attention_mask'].cpu()\n",
    "    input_id = model_input[\"input_ids\"].squeeze(1).cpu()\n",
    "\n",
    "    output = model_new(input_id, mask)\n",
    "    prob = torch.nn.functional.softmax(output, dim=1)[0]\n",
    "    pred_fact = output.argmax(dim=1).item()\n",
    "    # 0 for false, 1 for true\n",
    "    return pred_fact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2fe3a34c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# fact_eval(example_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "553e5560",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def pos_logit_to_reward(logit, task):\n",
    "    \"\"\"\n",
    "    Take the positive sentiment logit and scale it for the task.\n",
    "        task [negative]: reward = -logit\n",
    "        task [positive]: reward = logit\n",
    "    \"\"\"\n",
    "    for i in range(len(logit)):\n",
    "        if task[i]=='[false]':\n",
    "            logit[i] = -logit[i]\n",
    "        elif task[i]=='[true]':\n",
    "            pass\n",
    "        else:\n",
    "            raise ValueError('task has to be in [0, 1, 2]!')\n",
    "    return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "84f523ae",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### KG check to modify reward value \n",
    "\n",
    "# given a range i.e. i*fbs to (i+1)*fbs\n",
    "# should output a tensor([ 0, 0,  1, 1], device='cuda:0')\n",
    "\n",
    "# pred is the reponse in batch_response\n",
    "# query is the query_list\n",
    "\n",
    "def KG_check(lower, upper, batch_response):\n",
    "    factuals = []\n",
    "    for i in range(lower,upper):\n",
    "        query = DBpedia.loc[i,\"query\"]\n",
    "        # response = DBpedia.loc[i,\"response\"]\n",
    "        response = batch_response[i]\n",
    "        text = \"\" + query + response\n",
    "        # print(text)\n",
    "        factual = fact_eval(text)\n",
    "        factuals.append(factual)\n",
    "    # add variant \n",
    "    tensor_array = [i + random.random() for i in factuals ]\n",
    "    return torch.cuda.LongTensor(tensor_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3a4b589c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# KG_check(3, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1986dd29",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                          | 0/100 [00:00<?, ?it/s]C:\\Users\\wz\\AppData\\Local\\Temp\\ipykernel_24320\\655798921.py:21: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  return torch.cuda.LongTensor(tensor_array)\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [4:15:23<00:00, 153.24s/it]\n"
     ]
    }
   ],
   "source": [
    "# batch_size: 32\n",
    "# forward_batch_size: 4 \n",
    "\n",
    "ppo_trainer = PPOTrainer(gpt2_model, gpt2_model_ref, **config)\n",
    "fbs = config['forward_batch_size']\n",
    "\n",
    "for epoch in tqdm(range(int(np.ceil(config[\"steps\"]/config['batch_size'])))):\n",
    "    torch.cuda.empty_cache()\n",
    "    logs = dict()\n",
    "    game_data = dict()\n",
    "    timing = dict()\n",
    "    t0 = time.time()\n",
    "    \n",
    "    #### get a batch from the dataset and annotate tasks\n",
    "    df_batch = df.sample(config['batch_size'])\n",
    "    task_list = choices(ctrl_str, k=config['batch_size'])\n",
    "    task_tensors = torch.stack([ctrl_tokens[t] for t in task_list])\n",
    "    query_list = df_batch['query'].tolist()\n",
    "    game_data['query'] = [t+q for t,q in zip(task_list, query_list)]\n",
    "    \n",
    "    \n",
    "    #print(task_tensors)\n",
    "    query_tensors = torch.stack((df_batch['tokens'].tolist()))\n",
    "    #print(\"before \",query_tensors)\n",
    "    query_tensors = torch.cat((task_tensors, query_tensors), axis=1)\n",
    "\n",
    "    \n",
    "    #### get response from gpt2\n",
    "    t = time.time()\n",
    "    total_length = config['txt_in_len']+config['txt_out_len']\n",
    "    response_tensors_array = []\n",
    "    for i in range(int(config['batch_size']/fbs)):\n",
    "        response  = respond_to_batch(gpt2_model, query_tensors[i*fbs:(i+1)*fbs],\n",
    "                                     txt_len=config['txt_out_len'])\n",
    "        response_tensors_array.append(response)\n",
    "    response_tensors = torch.cat(response_tensors_array)\n",
    "#         #print(int(config['batch_size']/fbs))\n",
    "#         # 8 loops in total\n",
    "#         # 0-4, 5-8, 9-12...\n",
    "        \n",
    "#         # this is troublesome \n",
    "#         # error: probability tensor contains either `inf`, `nan` or element < 0\n",
    "#         #  next_token = torch.multinomial(probs, num_samples=1).squeeze(1) in gpt2.py function respond_to_batch\n",
    "        \n",
    "#         # response  = respond_to_batch(gpt2_model, query_tensors[i*fbs:(i+1)*fbs],txt_len=config['txt_out_len'])\n",
    "#         # we want a response of size [i*fbs:(i+1)*fbs]*1\n",
    "#         response_tensors = []\n",
    "#         for i in range(i*fbs, (i+1)*fbs):\n",
    "            \n",
    "#             seq = query_list[i]\n",
    "#             inpts = toker(seq, return_tensors=\"pt\")\n",
    "#             inpt_ids = inpts[\"input_ids\"]  # just IDS, no attn mask\n",
    "            \n",
    "#             key, loss = next_word_GPT2(i,100)\n",
    "            \n",
    "#             pred_id = base_tokenizer.encode(key, return_tensors = 'pt')\n",
    "#             # now it force response list into one tensor\n",
    "#             response = (pred_id[0])[1] # todo change it  should be one id not a list\n",
    "#             # print(response)\n",
    "#             response_tensors.append(response.item())\n",
    "#         response_tensors = torch.cuda.LongTensor(response_tensors)\n",
    "#         response_tensors_array.append(response_tensors)\n",
    "   \n",
    "#     # response_tensors.size = 32 * 1  \n",
    "#     response_tensors = torch.cat(response_tensors_array,axis = -1 )\n",
    "#     response_tensors  = torch.reshape(response_tensors, (-1, 1)) # force size of response_tensors.size = 32 * 1  rather that 32 length list\n",
    "#     # print(response_tensors)\n",
    "#     # print([gpt2_tokenizer.decode(response_tensors[i, :]) for i in range(config['batch_size'])])\n",
    "#     # print(response_tensors)\n",
    "    batch_response = [gpt2_tokenizer.decode(response_tensors[i, :]) for i in range(config['batch_size'])] # list of length 32\n",
    "    game_data['response'] = batch_response\n",
    "    timing['time/get_response'] = time.time()-t\n",
    "    #print( game_data['response'])\n",
    "\n",
    "    #### tokenize text for sentiment analysis\n",
    "    t = time.time()\n",
    "    texts = [q + r for q,r in zip(query_list, game_data['response'])]\n",
    "    sentiment_inputs, attention_masks = build_bert_batch_from_txt(texts, sentiment_tokenizer, device)    \n",
    "    timing['time/build_input_sentiment_unused'] = time.time()-t\n",
    "        \n",
    "    #### get KG score\n",
    "    t = time.time()\n",
    "    pos_logits = []\n",
    "    for i in range(int(config['batch_size']/fbs)):\n",
    "        device = torch.device(\"cuda\")\n",
    "        # modify the rewards based on the KG\n",
    "        res = KG_check(i*fbs, (i+1)*fbs, batch_response)\n",
    "        pos_logits.append(res)\n",
    "    \n",
    "    rewards = pos_logit_to_reward(torch.cat(pos_logits), task_list)\n",
    "    timing['time/get_factual_preds'] = time.time()-t\n",
    "\n",
    "\n",
    "    #### Run PPO training \n",
    "    t = time.time()\n",
    "    stats = ppo_trainer.step(query_tensors, response_tensors, rewards)\n",
    "    \n",
    "    # enforce nan to zero for wandb to run\n",
    "    stats = np.nan_to_num(stats)\n",
    "    # print(stats)\n",
    "    timing['time/optimization'] = time.time()-t\n",
    "     \n",
    "    #### Log everything\n",
    "    timing['time/epoch'] = time.time()-t0\n",
    "    table_rows = [list(r) for r in zip(game_data['query'], game_data['response'], rewards.cpu().tolist())]\n",
    "    logs.update({'game_log':wandb.Table(\n",
    "        columns=['query', 'response', 'reward'],\n",
    "        rows=table_rows)})\n",
    "    logs.update(timing)\n",
    "    # comment to remove the nan issues\n",
    "    logs.update(stats)\n",
    "    \n",
    "    # only support for float tensor\n",
    "    rewards = rewards.type(torch.FloatTensor)\n",
    "    \n",
    "    logs['env/reward_mean'] = torch.mean((rewards)).cpu().numpy()\n",
    "    logs['env/reward_std'] = torch.std(rewards).cpu().numpy()\n",
    "    logs['env/reward_dist'] = rewards.cpu().numpy()\n",
    "    for ctrl_s in ctrl_str:\n",
    "        key = 'env/reward_'+ctrl_s.strip('[]')\n",
    "        logs[key] = np.mean([r for r, t in zip(logs['env/reward_dist'], task_list) if t==ctrl_s])\n",
    "#     WandbCallback(log=None)\n",
    "    wandb.log(logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147a8b0f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a92e4f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
