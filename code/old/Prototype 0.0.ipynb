{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import random\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import relevant LMs package\n",
    "\n",
    "import transformers\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "from transformers import BertForSequenceClassification, AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import f1_score, accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use gpu\n"
     ]
    }
   ],
   "source": [
    "# cheak to use gpu or cpu\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Use gpu\")\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    print(\"Use cpu\")\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 30000)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 0\n",
    "t = 10000\n",
    "a = (1-np.random.rand(t,30000))*9\n",
    "np.shape(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = np.mean(a,axis=1)\n",
    "m\n",
    "np.sum(np.abs(m-4.5)<1/12)/t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KGs\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "import bs4\n",
    "import requests\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "from spacy.matcher import Matcher \n",
    "from spacy.tokens import Span \n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in 'C:\\\\Users\\\\wz\\\\Desktop\\\\context-aware-embedding-master\\\\Resources\\\\code': ['.DS_Store', '.ipynb_checkpoints', 'accuracy.py', 'Fun online example.ipynb', 'input', 'Messing around.ipynb', 'Prototype 0.0.ipynb', 'Prototype0.1.ipynb', 'query_trl.ipynb', 'test_trainer', 'trl', 'wandb', 'yago2', '__pycache__']\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/wz/IdeaProjects/context-aware-embedding/Resources/code/input/wiki_sentences_v2.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m files \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mlistdir(cwd)  \u001b[38;5;66;03m# Get all the files in that directory\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFiles in \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (cwd, files))\n\u001b[1;32m----> 9\u001b[0m candidate_sentences \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/Users/wz/IdeaProjects/context-aware-embedding/Resources/code/input/wiki_sentences_v2.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m candidate_sentences\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[0;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[0;32m    310\u001b[0m     )\n\u001b[1;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:680\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    665\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    666\u001b[0m     dialect,\n\u001b[0;32m    667\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    676\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m    677\u001b[0m )\n\u001b[0;32m    678\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 680\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:575\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    572\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    574\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 575\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    578\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:933\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    930\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    932\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 933\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1217\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1213\u001b[0m     mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1214\u001b[0m \u001b[38;5;66;03m# error: No overload variant of \"get_handle\" matches argument types\u001b[39;00m\n\u001b[0;32m   1215\u001b[0m \u001b[38;5;66;03m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[39;00m\n\u001b[0;32m   1216\u001b[0m \u001b[38;5;66;03m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[39;00m\n\u001b[1;32m-> 1217\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[0;32m   1218\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1219\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1220\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1222\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1223\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1224\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1225\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1226\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1227\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1228\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py:789\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    784\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    785\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    786\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    787\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    788\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 789\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    796\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    797\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    798\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/wz/IdeaProjects/context-aware-embedding/Resources/code/input/wiki_sentences_v2.csv'"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "# import wikipedia sentences\n",
    "import os\n",
    "\n",
    "cwd = os.getcwd()  # Get the current working directory (cwd)\n",
    "files = os.listdir(cwd)  # Get all the files in that directory\n",
    "print(\"Files in %r: %s\" % (cwd, files))\n",
    "\n",
    "candidate_sentences = pd.read_csv(\"/Users/wz/IdeaProjects/context-aware-embedding/Resources/code/input/wiki_sentences_v2.csv\")\n",
    "candidate_sentences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_sentences['sentence'].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"the drawdown process is governed by astm standard d823\")\n",
    "\n",
    "for tok in doc:\n",
    "    print(tok.text, \"...\", tok.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entities(sent):\n",
    "## chunk 1\n",
    "  ent1 = \"\"\n",
    "  ent2 = \"\"\n",
    "\n",
    "  prv_tok_dep = \"\"    # dependency tag of previous token in the sentence\n",
    "  prv_tok_text = \"\"   # previous token in the sentence\n",
    "\n",
    "  prefix = \"\"\n",
    "  modifier = \"\"\n",
    "\n",
    "  #############################################################\n",
    "  \n",
    "  for tok in nlp(sent):\n",
    "    ## chunk 2\n",
    "    # if token is a punctuation mark then move on to the next token\n",
    "    if tok.dep_ != \"punct\":\n",
    "      # check: token is a compound word or not\n",
    "      if tok.dep_ == \"compound\":\n",
    "        prefix = tok.text\n",
    "        # if the previous word was also a 'compound' then add the current word to it\n",
    "        if prv_tok_dep == \"compound\":\n",
    "          prefix = prv_tok_text + \" \"+ tok.text\n",
    "      \n",
    "      # check: token is a modifier or not\n",
    "      if tok.dep_.endswith(\"mod\") == True:\n",
    "        modifier = tok.text\n",
    "        # if the previous word was also a 'compound' then add the current word to it\n",
    "        if prv_tok_dep == \"compound\":\n",
    "          modifier = prv_tok_text + \" \"+ tok.text\n",
    "      \n",
    "      ## chunk 3\n",
    "      if tok.dep_.find(\"subj\") == True:\n",
    "        ent1 = modifier +\" \"+ prefix + \" \"+ tok.text\n",
    "        prefix = \"\"\n",
    "        modifier = \"\"\n",
    "        prv_tok_dep = \"\"\n",
    "        prv_tok_text = \"\"      \n",
    "\n",
    "      ## chunk 4\n",
    "      if tok.dep_.find(\"obj\") == True:\n",
    "        ent2 = modifier +\" \"+ prefix +\" \"+ tok.text\n",
    "        \n",
    "      ## chunk 5  \n",
    "      # update variables\n",
    "      prv_tok_dep = tok.dep_\n",
    "      prv_tok_text = tok.text\n",
    "  #############################################################\n",
    "\n",
    "  return [ent1.strip(), ent2.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_entities(\"the film had 200 patents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_pairs = []\n",
    "\n",
    "for i in tqdm(candidate_sentences[\"sentence\"]):\n",
    "    entity_pairs.append(get_entities(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_pairs[10:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relation(sent):\n",
    "    doc = nlp(sent)\n",
    "\n",
    "    # Matcher class object \n",
    "    matcher = Matcher(nlp.vocab)\n",
    "\n",
    "    #define the pattern \n",
    "    pattern = [{'DEP':'ROOT'}, \n",
    "                {'DEP':'prep','OP':\"?\"},\n",
    "                {'DEP':'agent','OP':\"?\"},  \n",
    "                {'POS':'ADJ','OP':\"?\"}] \n",
    "\n",
    "    matcher.add(\"matching_1\",[pattern],on_match=None ) \n",
    "\n",
    "    matches = matcher(doc)\n",
    "    k = len(matches) -1\n",
    "\n",
    "    span = doc[matches[k][1]:matches[k][2]] \n",
    "\n",
    "    return(span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_relation(\"John completed the task\")\n",
    "len(candidate_sentences[\"sentence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  256 th  和 2700 th sentence 有问题 跳过这句 \n",
    "# for i in tqdm(candidate_sentences['sentence'][257:2000]):\n",
    "#     get_relation(i)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relations = [get_relation(i) for i in tqdm(candidate_sentences['sentence'][270:2700]) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(relations).value_counts()[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract subject\n",
    "source = [i[0] for i in entity_pairs][270:2700]\n",
    "\n",
    "# extract object\n",
    "target = [i[1] for i in entity_pairs][270:2700]\n",
    "\n",
    "# print(len(source))\n",
    "# print(len(target))\n",
    "# print(len(relations))\n",
    "\n",
    "kg_df = pd.DataFrame({'source':source, 'target':target, 'edge':relations})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contains(source, target, relations):\n",
    "    triple = np.array([source, target, relations])\n",
    "    return (kg_df==triple).all(1).any()\n",
    "\n",
    "contains(\"connie\", \"own\",  \"decides\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a directed-graph from a dataframe\n",
    "G=nx.from_pandas_edgelist(kg_df, \"source\", \"target\", \n",
    "                          edge_attr=True, create_using=nx.MultiDiGraph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,12))\n",
    "\n",
    "pos = nx.spring_layout(G)\n",
    "nx.draw(G, with_labels=True, node_color='skyblue', edge_cmap=plt.cm.Blues, pos = pos)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract from YAGO2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colnames=['subject', 'predicate', 'object'] \n",
    "# YAGO = pd.read_csv(\"/Users/wz/IdeaProjects/context-aware-embedding/Resources/code/yago2/yago2core_facts.clean.notypes.tsv\" , names=colnames, header=None,sep ='\\t')\n",
    "YAGO = pd.read_csv(\"yago2\\\\yago2core_facts.clean.notypes.tsv\" , names=colnames, header=None,sep ='\\t')\n",
    "YAGO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove useless lines\n",
    "# 1-260: xxx http://www.w3.org/2000/01/rdf-schema <yago>\n",
    "YAGO = YAGO.iloc[261:557]\n",
    "# remove the <> sign\n",
    "YAGO['subject'] = YAGO['subject'].apply(lambda x: x[1:-1])\n",
    "YAGO['predicate'] = YAGO['predicate'].apply(lambda x: x[1:-1])\n",
    "YAGO['object'] = YAGO['object'].apply(lambda x: x[1:-1])\n",
    "YAGO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import wandb\n",
    "import time\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from random import choices\n",
    "import matplotlib.pyplot as plt\n",
    "tqdm.pandas()\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import GPT2Tokenizer\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "from trl.gpt2 import GPT2HeadWithValueModel, respond_to_batch\n",
    "from trl.ppo import PPOTrainer\n",
    "from trl.core import build_bert_batch_from_txt\n",
    "\n",
    "config = {\n",
    "    \"lm_name\": \"lvwerra/gpt2-imdb\",\n",
    "    \"ref_lm_name\": \"lvwerra/gpt2-imdb\",\n",
    "    \"cls_model_name\": \"lvwerra/distilbert-imdb\",\n",
    "    \"tk_name\": \"gpt2\",\n",
    "    \"steps\": 320,\n",
    "    \"batch_size\": 32,\n",
    "    \"forward_batch_size\": 4,\n",
    "    \"ppo_epochs\": 4,   \n",
    "    \"txt_in_len\": 2,\n",
    "    \"txt_out_len\": 1,\n",
    "    \"lr\": 1.41e-5,\n",
    "    \"init_kl_coef\":0.2,\n",
    "    \"target\": 6,\n",
    "    \"horizon\":10000,\n",
    "    \"gamma\":1,\n",
    "    \"lam\":0.95,\n",
    "    \"cliprange\": .2,\n",
    "    \"cliprange_value\":.2,\n",
    "    \"vf_coef\":.1, \n",
    "    \"seed\": 1,\n",
    "}\n",
    "\n",
    "np.random.seed(config['seed'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trails\n",
    "wandb.init(name='trails #1', project='gpt2-ctrl', config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pre-trained GPT2 language models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2_model = GPT2HeadWithValueModel.from_pretrained(config['lm_name'])\n",
    "gpt2_model_ref = GPT2HeadWithValueModel.from_pretrained(config['ref_lm_name'])\n",
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained(config['tk_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = gpt2_model.to(device)\n",
    "_ = gpt2_model_ref.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.watch(gpt2_model, log='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load BERT classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\"\n",
    "\n",
    "sentiment_model = AutoModelForSequenceClassification.from_pretrained(config[\"cls_model_name\"])\n",
    "sentiment_tokenizer = AutoTokenizer.from_pretrained(config[\"cls_model_name\"])\n",
    "\n",
    "# model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'this movie was really bad!!'\n",
    "output = sentiment_model.forward(sentiment_tokenizer.encode(text, return_tensors=\"pt\"))\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output[0][0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Token Prediction with GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "\n",
    "print(\"\\nBegin next-word using HF GPT-2 demo \")\n",
    "\n",
    "toker = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = \"connie own\"\n",
    "print(\"\\nInput sequence: \")\n",
    "print(seq)\n",
    "\n",
    "inpts = toker(seq, return_tensors=\"pt\")\n",
    "print(\"\\nTokenized input data structure: \")\n",
    "print(inpts)\n",
    "\n",
    "inpt_ids = inpts[\"input_ids\"]  # just IDS, no attn mask\n",
    "print(\"\\nToken IDs and their words: \")\n",
    "for id in inpt_ids[0]:\n",
    "    word = toker.decode(id)\n",
    "    print(id, word)\n",
    "\n",
    "    \n",
    "with torch.no_grad():\n",
    "    logits = model(**inpts).logits[:, -1, :]\n",
    "\n",
    "print(\"\\nAll logits for next word: \")\n",
    "print(logits)\n",
    "print(logits.shape)\n",
    "\n",
    "pred_id = torch.argmax(logits).item()\n",
    "print(\"\\nPredicted token ID of next word: \")\n",
    "print(pred_id)\n",
    "\n",
    "pred_word = toker.decode(pred_id)\n",
    "print(\"\\nPredicted next word for sequence: \")\n",
    "print(pred_word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GPT_pred(seq):\n",
    "    inpts = toker(seq, return_tensors=\"pt\")\n",
    "    inpt_ids = inpts[\"input_ids\"]  # just IDS, no attn mask\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inpts).logits[:, -1, :]\n",
    "    pred_id = torch.argmax(logits).item()\n",
    "    pred_word = toker.decode(pred_id)\n",
    "    return logits, pred_id, pred_word\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_logit_to_reward(logit, task):\n",
    "    \"\"\"\n",
    "    Take the positive sentiment logit and scale it for the task.\n",
    "        task [negative]: reward = -logit\n",
    "        task [positive]: reward = logit\n",
    "    \"\"\"\n",
    "    for i in range(len(logit)):\n",
    "        if task[i]=='[false]':\n",
    "            logit[i] = -logit[i]\n",
    "        elif task[i]=='[true]':\n",
    "            pass\n",
    "        else:\n",
    "            raise ValueError('task has to be in [0, 1, 2]!')\n",
    "    return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = kg_df[:1000]\n",
    "\n",
    "# device = torch.device(\"cpu\")\n",
    "#model(img_test.unsqueeze(0).cuda()).detach().cpu().clone().numpy()\n",
    "\n",
    "# modify query as object + relation\n",
    "# e.g connie owns \n",
    "df['query'] = df.apply(\n",
    "    lambda row: row.source + ' ' + row.edge, axis=1)\n",
    "# adjust format in list \n",
    "df['query'] = [df['query'][i].split()[:2] for i in range(1000)]\n",
    "df['query'] = df['query'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "#special token\n",
    "df['tokens'] = df['query'].apply(lambda x: gpt2_tokenizer.encode(' '+str(x), return_tensors=\"pt\").to(device))\n",
    "df['tokens'] = df['tokens'].apply(lambda x: x[0])\n",
    "df['tokens'] = df['tokens'].apply(lambda x: x[:2])\n",
    "\n",
    "# dropping invalid length in token column\n",
    "drop_row_index = []\n",
    "for i in range(1000):\n",
    "\n",
    "    if len(df['tokens'].loc[i]) < 2:\n",
    "        #df['tokens'].loc[i] = df['tokens'].loc[0].cuda().detach().cpu().clone().numpy()\n",
    "        drop_row_index.append(i)\n",
    "print(drop_row_index)\n",
    "df = df.drop(df.index[drop_row_index])\n",
    "        \n",
    "\n",
    "        \n",
    "# df.loc[(len(df['tokens']) < 2), 'tokens'] = df['tokens'].loc[0]\n",
    "#df['tokens'] = df['tokens'].apply(lambda x: df['tokens'].loc[0] if len(x) < 2)\n",
    "\n",
    "\n",
    "\n",
    "ctrl_str = ['[false]', '[true]']\n",
    "ctrl_tokens = dict((s, gpt2_tokenizer.encode(s, return_tensors=\"pt\").squeeze().to(device)) for s in ctrl_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PPO & Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda\")\n",
    "# device = \"cuda:0\"\n",
    "#input = input.to(device)\n",
    "#         model = model.to(device)\n",
    "\n",
    "a = [0,0,1,0]\n",
    "a = [i + random.random() for i in a ]\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### KG check to modify reward value \n",
    "\n",
    "# given a range i.e. i*fbs to (i+1)*fbs\n",
    "# should output a tensor([ 0, 0,  1, 1], device='cuda:0')\n",
    "\n",
    "# pred is the reponse in batch_response\n",
    "# query is the query_list\n",
    "\n",
    "def KG_check(lower, upper):\n",
    "    tensor_array = []\n",
    "    for i in range(lower,upper):\n",
    "        # e.g. dark knight not\n",
    "#         print('lower is ',lower )\n",
    "#         print('upper is ',upper )\n",
    "#         print(\"i is \", i)\n",
    "#         print(len(query_list))\n",
    "#         print(len(batch_response))\n",
    "\n",
    "        # bugs here todo query_list[i].split() could be one element only\n",
    "        source = query_list[i].split()[0]\n",
    "        target = batch_response[i]\n",
    "        if len(query_list[i].split()) <2:\n",
    "            \n",
    "            relation = 'anything'\n",
    "        else:\n",
    "            relation = query_list[i].split()[1]\n",
    "        \n",
    "#         print(i)\n",
    "#         print(\"source is \",source)\n",
    "#         print(\"target is \",target)\n",
    "#         print(\"relation is \",relation)\n",
    "        if contains(source, target, relation):\n",
    "            tensor_array.append(1)\n",
    "        else:\n",
    "            tensor_array.append(0)\n",
    "            \n",
    "    # rand modular Todo: Fix this\n",
    "    c = random.randint(0, 3)\n",
    "    tensor_array[c]=1\n",
    "    \n",
    "    # add variant \n",
    "    tensor_array = [i + random.random() for i in tensor_array ]\n",
    "    return torch.cuda.LongTensor(tensor_array)\n",
    "\n",
    "# KG_check(28,32)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "config = {\n",
    "    \"lm_name\": \"lvwerra/gpt2-imdb\",\n",
    "    \"ref_lm_name\": \"lvwerra/gpt2-imdb\",\n",
    "    \"cls_model_name\": \"lvwerra/distilbert-imdb\",\n",
    "    \"tk_name\": \"gpt2\",\n",
    "    \"steps\": 320,\n",
    "    \"batch_size\": 32,\n",
    "    \"forward_batch_size\": 4,\n",
    "    \"ppo_epochs\": 4,   \n",
    "    \"txt_in_len\": 2,\n",
    "    \"txt_out_len\": 1,\n",
    "    \"lr\": 1.41e-5,\n",
    "    \"init_kl_coef\":0.2,\n",
    "    \"target\": 6,\n",
    "    \"horizon\":10000,\n",
    "    \"gamma\":1,\n",
    "    \"lam\":0.95,\n",
    "    \"cliprange\": .2,\n",
    "    \"cliprange_value\":.2,\n",
    "    \"vf_coef\":.1, \n",
    "    \"seed\": 1,\n",
    "}\n",
    "\n",
    "# where is lvwerra/distilbert-imdb?????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug\n",
    "\n",
    "# df_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug\n",
    "\n",
    "# df_results = pd.DataFrame(game_data)\n",
    "# df_results\n",
    "#query_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for testing\n",
    "\n",
    "# batch_size: 32\n",
    "# forward_batch_size: 4 \n",
    "\n",
    "ppo_trainer = PPOTrainer(gpt2_model, gpt2_model_ref, **config)\n",
    "fbs = config['forward_batch_size']\n",
    "\n",
    "for epoch in tqdm(range(int(np.ceil(config[\"steps\"]/config['batch_size'])))):\n",
    "    torch.cuda.empty_cache()\n",
    "    logs = dict()\n",
    "    game_data = dict()\n",
    "    timing = dict()\n",
    "    t0 = time.time()\n",
    "    \n",
    "    #### get a batch from the dataset and annotate tasks\n",
    "    df_batch = df.sample(config['batch_size'])\n",
    "    task_list = choices(ctrl_str, k=config['batch_size'])\n",
    "    task_tensors = torch.stack([ctrl_tokens[t] for t in task_list])\n",
    "    query_list = df_batch['query'].tolist()\n",
    "    game_data['query'] = [t+q for t,q in zip(task_list, query_list)]\n",
    "    \n",
    "    \n",
    "    #print(task_tensors)\n",
    "    query_tensors = torch.stack((df_batch['tokens'].tolist()))\n",
    "    #print(\"before \",query_tensors)\n",
    "    query_tensors = torch.cat((task_tensors, query_tensors), axis=1)\n",
    "\n",
    "    \n",
    "    #### get response from gpt2\n",
    "    t = time.time()\n",
    "    response_tensors_array = []\n",
    "    for i in range(int(config['batch_size']/fbs)):\n",
    "        #print(int(config['batch_size']/fbs))\n",
    "        # 8 loops in total\n",
    "        # 0-4, 5-8, 9-12...\n",
    "        \n",
    "        # this is troublesome \n",
    "        # error: probability tensor contains either `inf`, `nan` or element < 0\n",
    "        #  next_token = torch.multinomial(probs, num_samples=1).squeeze(1) in gpt2.py function respond_to_batch\n",
    "        \n",
    "        # response  = respond_to_batch(gpt2_model, query_tensors[i*fbs:(i+1)*fbs],txt_len=config['txt_out_len'])\n",
    "        # we want a response of size [i*fbs:(i+1)*fbs]*1\n",
    "        response_tensors = []\n",
    "        for i in range(i*fbs, (i+1)*fbs):\n",
    "            seq = query_list[i]\n",
    "            inpts = toker(seq, return_tensors=\"pt\")\n",
    "            inpt_ids = inpts[\"input_ids\"]  # just IDS, no attn mask\n",
    "            for id in inpt_ids[0]:\n",
    "                word = toker.decode(id)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                logits = model(**inpts).logits[:, -1, :]\n",
    "\n",
    "            pred_id = torch.argmax(logits)\n",
    "        \n",
    "            response = pred_id\n",
    "            \n",
    "            response_tensors.append(response.item())\n",
    "        response_tensors = torch.cuda.LongTensor(response_tensors)\n",
    "        response_tensors_array.append(response_tensors)\n",
    "   \n",
    "    # response_tensors.size = 32 * 1  \n",
    "    response_tensors = torch.cat(response_tensors_array,axis = -1 )\n",
    "    response_tensors  = torch.reshape(response_tensors, (-1, 1)) # force size of response_tensors.size = 32 * 1  rather that 32 length list\n",
    "    #print(response_tensors)\n",
    "    #print([gpt2_tokenizer.decode(response_tensors[i, :]) for i in range(config['batch_size'])])\n",
    "    batch_response = [gpt2_tokenizer.decode(response_tensors[i, :]) for i in range(config['batch_size'])] # list of length 32\n",
    "    game_data['response'] = batch_response\n",
    "    timing['time/get_response'] = time.time()-t\n",
    "    #print( game_data['response'])\n",
    "\n",
    "    #### tokenize text for sentiment analysis\n",
    "    t = time.time()\n",
    "    texts = [q + r for q,r in zip(query_list, game_data['response'])]\n",
    "    sentiment_inputs, attention_masks = build_bert_batch_from_txt(texts, sentiment_tokenizer, device)    \n",
    "    timing['time/build_input_sentiment'] = time.time()-t\n",
    "        \n",
    "    #### get sentiment score\n",
    "    t = time.time()\n",
    "    pos_logits = []\n",
    "    for i in range(int(config['batch_size']/fbs)):\n",
    "        input = {'input_ids':      sentiment_inputs[i*fbs:(i+1)*fbs],\n",
    "                  'attention_mask': attention_masks[i*fbs:(i+1)*fbs],\n",
    "                  'labels':         None\n",
    "                 }      \n",
    "        #res = sentiment_model.forward(**input)[0][:, 1].detach()\n",
    "        \n",
    "        device = torch.device(\"cuda\")\n",
    "        sentiment_model = sentiment_model.to(device)\n",
    "        res = sentiment_model.forward(sentiment_inputs[i*fbs:(i+1)*fbs],\n",
    "                                      attention_masks[i*fbs:(i+1)*fbs])[0][:, 1].detach()\n",
    "#         print(res)\n",
    "#         print(res.size())\n",
    "\n",
    "        # modify the rewards based on the KG\n",
    "#         print('lower is ',i*fbs )\n",
    "#         print('upper is ',(i+1)*fbs )\n",
    "        res = KG_check(i*fbs, (i+1)*fbs)\n",
    "\n",
    "        pos_logits.append(res)\n",
    "    \n",
    "    rewards = pos_logit_to_reward(torch.cat(pos_logits), task_list)\n",
    "    timing['time/get_sentiment_preds'] = time.time()-t\n",
    "\n",
    "\n",
    "    #### Run PPO training \n",
    "    t = time.time()\n",
    "    stats = ppo_trainer.step(query_tensors, response_tensors, rewards)\n",
    "    \n",
    "    # todo fix this \n",
    "    # enforce nan to zero for wandb to run\n",
    "    stats = np.nan_to_num(stats)\n",
    "    #print(stats)\n",
    "    timing['time/optimization'] = time.time()-t\n",
    "     \n",
    "    #### Log everything\n",
    "    timing['time/epoch'] = time.time()-t0\n",
    "    table_rows = [list(r) for r in zip(game_data['query'], game_data['response'], rewards.cpu().tolist())]\n",
    "    logs.update({'game_log':wandb.Table(\n",
    "        columns=['query', 'response', 'reward'],\n",
    "        rows=table_rows)})\n",
    "    logs.update(timing)\n",
    "    # comment to remove the nan issues\n",
    "    #logs.update(stats)\n",
    "    \n",
    "    # only support for float tensor\n",
    "    rewards = rewards.type(torch.FloatTensor)\n",
    "    \n",
    "    logs['env/reward_mean'] = torch.mean((rewards)).cpu().numpy()\n",
    "    logs['env/reward_std'] = torch.std(rewards).cpu().numpy()\n",
    "    logs['env/reward_dist'] = rewards.cpu().numpy()\n",
    "    for ctrl_s in ctrl_str:\n",
    "        key = 'env/reward_'+ctrl_s.strip('[]')\n",
    "        logs[key] = np.mean([r for r, t in zip(logs['env/reward_dist'], task_list) if t==ctrl_s])\n",
    "#     WandbCallback(log=None)\n",
    "    wandb.log(logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug\n",
    "\n",
    "\n",
    "#     cuda0 = torch.device('cuda:0')\n",
    "\n",
    "#     for i in range(config['batch_size']):\n",
    "#         current_row = df_batch['tokens'].iloc[i]\n",
    "    \n",
    "#         if type(current_row) is np.ndarray:\n",
    "#             #print('before ', current_row[0])\n",
    "#             #print(i)\n",
    "#             val = [current_row[0], current_row[1]]\n",
    "#             df_batch['tokens'].iloc[i] = list((torch.cuda.LongTensor([val[0]]),torch.cuda.LongTensor([val[1]])))\n",
    "#             print(i)\n",
    "#             print(df_batch['tokens'].iloc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i = [6096, 9000]\n",
    "# list((torch.cuda.LongTensor(i[0]),torch.cuda.LongTensor(i[1])))\n",
    "# #torch.cpu.as_tensor(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### get a batch from the dataset\n",
    "bs = 32\n",
    "game_data = dict()\n",
    "df_batch = df.sample(bs)\n",
    "query_list = df_batch['query'].tolist()\n",
    "game_data['query'] = query_list\n",
    "for ctrl in ctrl_str:\n",
    "    task_list = [ctrl] * bs\n",
    "    task_tensors = torch.stack([ctrl_tokens[t] for t in task_list])\n",
    "\n",
    "    query_tensors = torch.stack(df_batch['tokens'].tolist())\n",
    "    query_tensors = torch.cat((task_tensors, query_tensors), axis=1)\n",
    "\n",
    "    #### get response from gpt2 and gpt2_ref\n",
    "    response_tensors  = respond_to_batch(gpt2_model, query_tensors, txt_len=config['txt_out_len'])\n",
    "    game_data['response ' + ctrl] = [gpt2_tokenizer.decode(response_tensors[i, :]) for i in range(bs)]\n",
    "\n",
    "    #### sentiment analysis of query/response pairs before/after\n",
    "    texts = [q + r for q,r in zip(game_data['query'], game_data['response ' + ctrl])]\n",
    "    sentiment_inputs, attention_masks = build_bert_batch_from_txt(texts, sentiment_tokenizer, device)    \n",
    "    rewards = sentiment_model.forward(sentiment_inputs, attention_masks)[0][:, 1].detach()\n",
    "    game_data['rewards ' + ctrl] = pos_logit_to_reward(rewards, task_list).cpu().numpy()\n",
    "\n",
    "# store results in a dataframe\n",
    "df_results = pd.DataFrame(game_data)\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use BERT tokenizer\n",
    "\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "# call tokenize.encode function to convert sentences\n",
    "\n",
    "# # original sentence(X)\n",
    "# print(' Original sentence: ', X[0])\n",
    "\n",
    "# # tokenized sentence(X)\n",
    "# print('Tokenized sentence: ', bert_tokenizer.tokenize(X[0]))\n",
    "\n",
    "# # map sentence to token IDs\n",
    "# print('Mapping to IDs: ', bert_tokenizer.convert_tokens_to_ids(bert_tokenizer.tokenize(X[0])))\n",
    "\n",
    "# add [CLS] [SEP] [PAD] token as required \n",
    "# The encode funtion will 1. split into tokens 2. add [CLS] and [SEP] 3. map tokens to IDs\n",
    "input_id = []\n",
    "for s in X:\n",
    "    encoded  = bert_tokenizer.encode(s,  # the sentence for encoding\n",
    "                                     add_special_tokens = True,  # [CLS] and [SEP]\n",
    "                                     max_length = 200, # maximum length of sentence\n",
    "                                     pad_to_max_length = True, # padding length\n",
    "                                     return_tensors = 'pt' # return type is tensor\n",
    "                                    )\n",
    "    input_id.append(encoded)\n",
    "\n",
    "\n",
    "input_id = torch.cat(input_id, dim=0)\n",
    "label_y = torch.tensor(Y) \n",
    "\n",
    "# set epoch and batch_size\n",
    "\n",
    "epoch = 4\n",
    "batch_size = 32\n",
    "\n",
    "# split data for training and validation\n",
    "data = TensorDataset(input_id,label_y)\n",
    "data_len = len(data)\n",
    "train, validation = random_split(data, [int(0.8*data_len), data_len - int(0.8*data_len)])\n",
    "\n",
    "# create DataLoader for both training set and valiadation set\n",
    "train_dl = DataLoader(train, batch_size = batch_size, shuffle = True)\n",
    "val_dl = DataLoader(validation, batch_size = batch_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use BertForSequenceClassification as our model\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', \n",
    "                                                      num_labels=4, \n",
    "                                                      output_attentions=False, # no attention weight\n",
    "                                                      output_hidden_states=False # not need to return all hidden state )\n",
    "                                                     )\n",
    "# use cpu to train\n",
    "model.cpu()\n",
    "\n",
    "# optimizer and lr schedule\n",
    "\n",
    "# convention is to use AdamW where W is weight decay fix from huggingface\n",
    "optim = AdamW(model.parameters(), lr = 2e-5)\n",
    "scheduler = get_linear_schedule_with_warmup(optim, \n",
    "                                           num_warmup_steps = 0, # default\n",
    "                                           num_training_steps = len(train_dl)*epoch)\n",
    "\n",
    "# define accuracy by accuracy_score\n",
    "\n",
    "def accuracy(pred_label, y):\n",
    "    pred_label = np.argmax(pred_label, axis =1).flatten()\n",
    "    return accuracy_score(pred_label, y.flatten())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the seed value to make sure the return are determistic\n",
    "\n",
    "seed_value = 488\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "torch.manual_seed(seed_value)\n",
    "torch.cuda.manual_seed_all(seed_value)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "device = torch.device('cpu')\n",
    "\n",
    "\n",
    "# training phase\n",
    "\n",
    "for epc in range(epoch):\n",
    "    # accumulated loss for training and validation\n",
    "    training_loss = 0\n",
    "    validation_loss = 0\n",
    "    evaluation_acc = 0\n",
    "    \n",
    "    # training mode\n",
    "    model.train()\n",
    "    for step, batch in enumerate(train_dl):\n",
    "        model.zero_grad()\n",
    "        # each batch has 3 tensors: 1. input id 2. attention mask 3. label\n",
    "        # e.g. batch[i].to(device)\n",
    "        # follow the documentation on https://huggingface.co/transformers\n",
    "        input_id = batch[0].to(device)\n",
    "        att_mask = (batch[0]>0).to(device)\n",
    "        input_label = batch[1].to(device)\n",
    "        output = model(input_id, token_type_ids=None, attention_mask=att_mask, labels=input_label)\n",
    "        loss = output[0]\n",
    "        print(\"training batch \",step,\" with the loss of \", loss.item())\n",
    "        training_loss += loss.item()\n",
    "        # backprop\n",
    "        loss.backward()\n",
    "        # clip the gradient to 1.0, prevent exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        # update parameter\n",
    "        optim.step() \n",
    "        # update lr\n",
    "        scheduler.step()\n",
    "        \n",
    "    # eval mode\n",
    "    model.eval()\n",
    "    for step, batch in enumerate(val_dl):\n",
    "        # indicate the model not to compute gradients !\n",
    "        with torch.no_grad():\n",
    "            input_id = batch[0].to(device)\n",
    "            att_mask = (batch[0]>0).to(device)\n",
    "            input_label = batch[1].to(device)\n",
    "            output = model(input_id, token_type_ids=None, attention_mask=att_mask, labels=input_label)\n",
    "            loss = output[0] \n",
    "            print(\"val batch \",step,\" with the loss of \", loss.item())\n",
    "            validation_loss += loss.item()\n",
    "            pred = output[1].detach().cpu().numpy()\n",
    "            ground_truth_label = batch[1].to('cpu').numpy()\n",
    "            evaluation_acc += accuracy(pred, ground_truth_label)\n",
    "            \n",
    "    print('Validation loss: ', validation_loss / len(val_dl))\n",
    "    print('Overall accuracy: ', evaluation_acc / len(val_dl))\n",
    "\n",
    "print('Training finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing\n",
    "# predicate test label\n",
    "\n",
    "# add [CLS] [SEP] [PAD] token as required\n",
    "input_id_test = []\n",
    "for s in Xt:\n",
    "    encoded  = bert_tokenizer.encode(s, add_special_tokens = True, max_length = 200, \n",
    "                                pad_to_max_length = True, return_tensors = 'pt')\n",
    "    input_id_test.append(encoded)\n",
    "    \n",
    "input_id_test = torch.cat(input_id_test, dim=0)\n",
    "test_data = TensorDataset(input_id_test)\n",
    "test_dl = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "pred_label = []\n",
    "for step, (batch,) in enumerate(test_dl):\n",
    "    if step % 10 == 0:\n",
    "        print(\"processed with batch \", step)\n",
    "    with torch.no_grad():\n",
    "        output = model(batch.to(device), token_type_ids=None, attention_mask=(batch>0).to(device))\n",
    "        pred = output[0].detach().cpu().numpy()\n",
    "        pred_label.append(pred)\n",
    "print('Test predicting finished')\n",
    "final_preds = np.concatenate(pred_label, axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert probabilites into labels\n",
    "Y_test_pred = final_preds\n",
    "\n",
    "Y_pred = np.zeros((Y_test_pred.shape[0],1))\n",
    "\n",
    "for i in range (Y_test_pred.shape[0]):\n",
    "    array = Y_test_pred[i]\n",
    "    \n",
    "    max_val = np.max(array)\n",
    "    \n",
    "    index_max = np.where(array == max_val)\n",
    "    Y_pred[i] = index_max[0][0]\n",
    "    \n",
    "    \n",
    "print(Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write out the csv file\n",
    "# first column is the id, it is the index to the list of test examples\n",
    "# second column is the predction as an integer\n",
    "fout = open(\"out.csv\", \"w\")\n",
    "fout.write(\"Id,Y\\n\")\n",
    "for i, line in enumerate(Y_pred): # Y_test_pred is in the same order as the test data\n",
    "    fout.write(\"%d,%d\\n\" % (i, line))\n",
    "fout.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### REFERENCE\n",
    "\n",
    "# https://www.kaggle.com/hassanamin/bert-pytorch-cola-classification\n",
    "# https://www.cnblogs.com/tangjianwei/p/13334327.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
