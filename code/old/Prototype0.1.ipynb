{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import random\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import relevant LMs package\n",
    "\n",
    "import transformers\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "from transformers import BertForSequenceClassification, AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import f1_score, accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use gpu\n"
     ]
    }
   ],
   "source": [
    "# cheak to use gpu or cpu\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Use gpu\")\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    print(\"Use cpu\")\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KGs\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "import bs4\n",
    "import requests\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "from spacy.matcher import Matcher \n",
    "from spacy.tokens import Span \n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in 'C:\\\\Users\\\\wz\\\\Desktop\\\\context-aware-embedding-master\\\\Resources\\\\code': ['.DS_Store', '.ipynb_checkpoints', 'accuracy.py', 'Fun online example.ipynb', 'input', 'Messing around.ipynb', 'Prototype 0.0.ipynb', 'Prototype0.1.ipynb', 'query_trl.ipynb', 'runs', 'test_trainer', 'trl', 'wandb', 'yago2', '__pycache__']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4318, 1)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "# import wikipedia sentences\n",
    "import os\n",
    "\n",
    "cwd = os.getcwd()  # Get the current working directory (cwd)\n",
    "files = os.listdir(cwd)  # Get all the files in that directory  \n",
    "print(\"Files in %r: %s\" % (cwd, files))\n",
    "\n",
    "candidate_sentences = pd.read_csv(\"\\\\Users\\\\wz\\\\Desktop\\\\context-aware-embedding-master\\\\Resources\\\\code\\\\input\\\\wiki_sentences_v2.csv\")\n",
    "candidate_sentences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1624    vocals were provided by trivedi, raftaar, ayushmann khurrana, arijit singh, abhijeet srivastava, aakansha sharma, shadab faridi and altamash faridi.\n",
       "2295                                                                                                          the trailer was released on december 22, 2011.\n",
       "3128                                                                                                                 majority of the fans considered his ra.\n",
       "636                                                                                                                      ii was released on august 28, 2009.\n",
       "3494                                                                                            educational and sports films are exempt from classification.\n",
       "Name: sentence, dtype: object"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidate_sentences['sentence'].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the ... det\n",
      "drawdown ... compound\n",
      "process ... nsubjpass\n",
      "is ... auxpass\n",
      "governed ... ROOT\n",
      "by ... agent\n",
      "astm ... compound\n",
      "standard ... pobj\n",
      "d823 ... punct\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"the drawdown process is governed by astm standard d823\")\n",
    "\n",
    "for tok in doc:\n",
    "    print(tok.text, \"...\", tok.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entities(sent):\n",
    "## chunk 1\n",
    "  ent1 = \"\"\n",
    "  ent2 = \"\"\n",
    "\n",
    "  prv_tok_dep = \"\"    # dependency tag of previous token in the sentence\n",
    "  prv_tok_text = \"\"   # previous token in the sentence\n",
    "\n",
    "  prefix = \"\"\n",
    "  modifier = \"\"\n",
    "\n",
    "  #############################################################\n",
    "  \n",
    "  for tok in nlp(sent):\n",
    "    ## chunk 2\n",
    "    # if token is a punctuation mark then move on to the next token\n",
    "    if tok.dep_ != \"punct\":\n",
    "      # check: token is a compound word or not\n",
    "      if tok.dep_ == \"compound\":\n",
    "        prefix = tok.text\n",
    "        # if the previous word was also a 'compound' then add the current word to it\n",
    "        if prv_tok_dep == \"compound\":\n",
    "          prefix = prv_tok_text + \" \"+ tok.text\n",
    "      \n",
    "      # check: token is a modifier or not\n",
    "      if tok.dep_.endswith(\"mod\") == True:\n",
    "        modifier = tok.text\n",
    "        # if the previous word was also a 'compound' then add the current word to it\n",
    "        if prv_tok_dep == \"compound\":\n",
    "          modifier = prv_tok_text + \" \"+ tok.text\n",
    "      \n",
    "      ## chunk 3\n",
    "      if tok.dep_.find(\"subj\") == True:\n",
    "        ent1 = modifier +\" \"+ prefix + \" \"+ tok.text\n",
    "        prefix = \"\"\n",
    "        modifier = \"\"\n",
    "        prv_tok_dep = \"\"\n",
    "        prv_tok_text = \"\"      \n",
    "\n",
    "      ## chunk 4\n",
    "      if tok.dep_.find(\"obj\") == True:\n",
    "        ent2 = modifier +\" \"+ prefix +\" \"+ tok.text\n",
    "        \n",
    "      ## chunk 5  \n",
    "      # update variables\n",
    "      prv_tok_dep = tok.dep_\n",
    "      prv_tok_text = tok.text\n",
    "  #############################################################\n",
    "\n",
    "  return [ent1.strip(), ent2.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['film', '200  patents']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_entities(\"the film had 200 patents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 4318/4318 [00:13<00:00, 331.77it/s]\n"
     ]
    }
   ],
   "source": [
    "entity_pairs = []\n",
    "\n",
    "for i in tqdm(candidate_sentences[\"sentence\"]):\n",
    "    entity_pairs.append(get_entities(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['we', 'tests'],\n",
       " ['m', 'international sales rights'],\n",
       " ['musician robbie robertson', 'soundtrack'],\n",
       " ['it', 'original music tracks'],\n",
       " ['it', 'reviewed  franchise'],\n",
       " ['she', 'accidentally  mystique'],\n",
       " ['military  forces', 'arrest'],\n",
       " ['train', 'vuk'],\n",
       " ['kota eberhardt', 'telepath selene gallio'],\n",
       " ['singer', '-']]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_pairs[10:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relation(sent):\n",
    "    doc = nlp(sent)\n",
    "\n",
    "    # Matcher class object \n",
    "    matcher = Matcher(nlp.vocab)\n",
    "\n",
    "    #define the pattern \n",
    "    pattern = [{'DEP':'ROOT'}, \n",
    "                {'DEP':'prep','OP':\"?\"},\n",
    "                {'DEP':'agent','OP':\"?\"},  \n",
    "                {'POS':'ADJ','OP':\"?\"}] \n",
    "\n",
    "    matcher.add(\"matching_1\",[pattern],on_match=None ) \n",
    "\n",
    "    matches = matcher(doc)\n",
    "    k = len(matches) -1\n",
    "\n",
    "    span = doc[matches[k][1]:matches[k][2]] \n",
    "\n",
    "    return(span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4318"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_relation(\"John completed the task\")\n",
    "len(candidate_sentences[\"sentence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  256 th  和 2700 th sentence 有问题 跳过这句 \n",
    "# for i in tqdm(candidate_sentences['sentence'][257:2000]):\n",
    "#     get_relation(i)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1430/1430 [00:04<00:00, 322.29it/s]\n"
     ]
    }
   ],
   "source": [
    "relations = [get_relation(i) for i in tqdm(candidate_sentences['sentence'][270:1700]) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "is               120\n",
       "was              109\n",
       "released on       43\n",
       "include           34\n",
       "are               29\n",
       "were              22\n",
       "released          16\n",
       "became            14\n",
       "composed by       14\n",
       "produced          12\n",
       "has               11\n",
       "have              11\n",
       "become            11\n",
       "made              10\n",
       "considered         8\n",
       "hired              8\n",
       "need               7\n",
       "included           7\n",
       "introduced in      7\n",
       "had                6\n",
       "be                 6\n",
       "been               6\n",
       "starred in         6\n",
       "produced by        5\n",
       "'s                 5\n",
       "written by         5\n",
       "scheduled          5\n",
       "directed by        5\n",
       "began              5\n",
       "used               5\n",
       "known as           5\n",
       "released in        5\n",
       "planned            5\n",
       "george             4\n",
       "given              4\n",
       "called             4\n",
       "hosted             4\n",
       "gives              4\n",
       "features           4\n",
       "reported           4\n",
       "cast as            4\n",
       "designed           4\n",
       "started in         4\n",
       "start              4\n",
       "rao                4\n",
       "starred            4\n",
       "began in           4\n",
       "portrays           4\n",
       "continued          3\n",
       "remade in          3\n",
       "dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(relations).value_counts()[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract subject\n",
    "source = [i[0] for i in entity_pairs][270:1700]\n",
    "\n",
    "# extract object\n",
    "target = [i[1] for i in entity_pairs][270:1700]\n",
    "\n",
    "# print(len(source))\n",
    "# print(len(target))\n",
    "# print(len(relations))\n",
    "\n",
    "kg_df = pd.DataFrame({'source':source, 'target':target, 'edge':relations})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "      <th>edge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>examples</td>\n",
       "      <td>unfriended friend request</td>\n",
       "      <td>include</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>one</td>\n",
       "      <td>noticeable side effects</td>\n",
       "      <td>is sleeplessness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>early horror films</td>\n",
       "      <td>social  controversy</td>\n",
       "      <td>created great</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>however horror movies</td>\n",
       "      <td>positive  endings</td>\n",
       "      <td>present positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>satirical comedy drama</td>\n",
       "      <td>illicit love affair</td>\n",
       "      <td>is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1425</th>\n",
       "      <td>popular  tune</td>\n",
       "      <td>audiences</td>\n",
       "      <td>believed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1426</th>\n",
       "      <td>bollywood</td>\n",
       "      <td>other  products</td>\n",
       "      <td>used</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1427</th>\n",
       "      <td>dual voting system</td>\n",
       "      <td>1956</td>\n",
       "      <td>developed in</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1428</th>\n",
       "      <td>national film awards</td>\n",
       "      <td>also  1954</td>\n",
       "      <td>introduced in</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1429</th>\n",
       "      <td>many  pakistanis</td>\n",
       "      <td>linguistic  similarity</td>\n",
       "      <td>understand</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1430 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      source                     target              edge\n",
       "0                   examples  unfriended friend request           include\n",
       "1                        one    noticeable side effects  is sleeplessness\n",
       "2         early horror films        social  controversy     created great\n",
       "3      however horror movies          positive  endings  present positive\n",
       "4     satirical comedy drama        illicit love affair                is\n",
       "...                      ...                        ...               ...\n",
       "1425           popular  tune                  audiences          believed\n",
       "1426               bollywood            other  products              used\n",
       "1427      dual voting system                       1956      developed in\n",
       "1428    national film awards                 also  1954     introduced in\n",
       "1429        many  pakistanis     linguistic  similarity        understand\n",
       "\n",
       "[1430 rows x 3 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def contains(source, target, relations):\n",
    "    triple = np.array([source, target, relations])\n",
    "    return (kg_df==triple).all(1).any()\n",
    "\n",
    "contains(\"connie\", \"own\",  \"decides\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a directed-graph from a dataframe\n",
    "G=nx.from_pandas_edgelist(kg_df, \"source\", \"target\", \n",
    "                          edge_attr=True, create_using=nx.MultiDiGraph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'scipy.sparse' has no attribute 'coo_array'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [21]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m12\u001b[39m,\u001b[38;5;241m12\u001b[39m))\n\u001b[1;32m----> 3\u001b[0m pos \u001b[38;5;241m=\u001b[39m \u001b[43mnx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspring_layout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mG\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m nx\u001b[38;5;241m.\u001b[39mdraw(G, with_labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, node_color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mskyblue\u001b[39m\u001b[38;5;124m'\u001b[39m, edge_cmap\u001b[38;5;241m=\u001b[39mplt\u001b[38;5;241m.\u001b[39mcm\u001b[38;5;241m.\u001b[39mBlues, pos \u001b[38;5;241m=\u001b[39m pos)\n\u001b[0;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\networkx\\utils\\decorators.py:845\u001b[0m, in \u001b[0;36margmap.__call__.<locals>.func\u001b[1;34m(_argmap__wrapper, *args, **kwargs)\u001b[0m\n\u001b[0;32m    844\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunc\u001b[39m(\u001b[38;5;241m*\u001b[39margs, __wrapper\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 845\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m argmap\u001b[38;5;241m.\u001b[39m_lazy_compile(__wrapper)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m<class 'networkx.utils.decorators.argmap'> compilation 8:4\u001b[0m, in \u001b[0;36margmap_spring_layout_5\u001b[1;34m(G, k, pos, fixed, iterations, threshold, weight, scale, center, dim, seed)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgzip\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01minspect\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mitertools\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\networkx\\drawing\\layout.py:476\u001b[0m, in \u001b[0;36mspring_layout\u001b[1;34m(G, k, pos, fixed, iterations, threshold, weight, scale, center, dim, seed)\u001b[0m\n\u001b[0;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(G) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m500\u001b[39m:  \u001b[38;5;66;03m# sparse solver for large graphs\u001b[39;00m\n\u001b[0;32m    475\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m\n\u001b[1;32m--> 476\u001b[0m A \u001b[38;5;241m=\u001b[39m \u001b[43mnx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_scipy_sparse_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mG\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m fixed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;66;03m# We must adjust k by domain size for layouts not near 1x1\u001b[39;00m\n\u001b[0;32m    479\u001b[0m     nnodes, _ \u001b[38;5;241m=\u001b[39m A\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\networkx\\convert_matrix.py:909\u001b[0m, in \u001b[0;36mto_scipy_sparse_array\u001b[1;34m(G, nodelist, dtype, weight, format)\u001b[0m\n\u001b[0;32m    906\u001b[0m     row, col, data \u001b[38;5;241m=\u001b[39m [], [], []\n\u001b[0;32m    908\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m G\u001b[38;5;241m.\u001b[39mis_directed():\n\u001b[1;32m--> 909\u001b[0m     A \u001b[38;5;241m=\u001b[39m \u001b[43msp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcoo_array\u001b[49m((data, (row, col)), shape\u001b[38;5;241m=\u001b[39m(nlen, nlen), dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    910\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    911\u001b[0m     \u001b[38;5;66;03m# symmetrize matrix\u001b[39;00m\n\u001b[0;32m    912\u001b[0m     d \u001b[38;5;241m=\u001b[39m data \u001b[38;5;241m+\u001b[39m data\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'scipy.sparse' has no attribute 'coo_array'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1200 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12,12))\n",
    "\n",
    "pos = nx.spring_layout(G)\n",
    "nx.draw(G, with_labels=True, node_color='skyblue', edge_cmap=plt.cm.Blues, pos = pos)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YAGO2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>predicate</th>\n",
       "      <th>object</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>Aristotle</td>\n",
       "      <td>influences</td>\n",
       "      <td>Ayn_Rand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>Aristotle</td>\n",
       "      <td>influences</td>\n",
       "      <td>Alexander_the_Great</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>Aristotle</td>\n",
       "      <td>influences</td>\n",
       "      <td>Avicenna</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>Aristotle</td>\n",
       "      <td>influences</td>\n",
       "      <td>Albertus_Magnus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>Aristotle</td>\n",
       "      <td>influences</td>\n",
       "      <td>Augustine_of_Hippo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>552</th>\n",
       "      <td>Angola</td>\n",
       "      <td>participatedIn</td>\n",
       "      <td>Battle_of_Cuito_Cuanavale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>553</th>\n",
       "      <td>Angola</td>\n",
       "      <td>participatedIn</td>\n",
       "      <td>South_African_Border_War</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>554</th>\n",
       "      <td>Angola</td>\n",
       "      <td>exports</td>\n",
       "      <td>wordnet_lumber_114943580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555</th>\n",
       "      <td>Angola</td>\n",
       "      <td>participatedIn</td>\n",
       "      <td>Battle_of_Kitombo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>556</th>\n",
       "      <td>Angola</td>\n",
       "      <td>participatedIn</td>\n",
       "      <td>Battle_of_Mbidizi_River</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>296 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       subject       predicate                     object\n",
       "261  Aristotle      influences                   Ayn_Rand\n",
       "262  Aristotle      influences        Alexander_the_Great\n",
       "263  Aristotle      influences                   Avicenna\n",
       "264  Aristotle      influences            Albertus_Magnus\n",
       "265  Aristotle      influences         Augustine_of_Hippo\n",
       "..         ...             ...                        ...\n",
       "552     Angola  participatedIn  Battle_of_Cuito_Cuanavale\n",
       "553     Angola  participatedIn   South_African_Border_War\n",
       "554     Angola         exports   wordnet_lumber_114943580\n",
       "555     Angola  participatedIn          Battle_of_Kitombo\n",
       "556     Angola  participatedIn    Battle_of_Mbidizi_River\n",
       "\n",
       "[296 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colnames=['subject', 'predicate', 'object'] \n",
    "YAGO = pd.read_csv(\"\\\\Users\\\\wz\\\\Desktop\\\\context-aware-embedding-master\\\\Resources\\\\code\\\\yago2\\\\yago2core_facts.clean.notypes.tsv\" , names=colnames, header=None,sep ='\\t')\n",
    "# remove useless lines\n",
    "# 1-260: xxx http://www.w3.org/2000/01/rdf-schema <yago>\n",
    "YAGO = YAGO.iloc[261:557]\n",
    "# remove the <> sign\n",
    "YAGO['subject'] = YAGO['subject'].apply(lambda x: x[1:-1])\n",
    "YAGO['predicate'] = YAGO['predicate'].apply(lambda x: x[1:-1])\n",
    "YAGO['object'] = YAGO['object'].apply(lambda x: x[1:-1])\n",
    "YAGO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetune against YAGO2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = YAGO\n",
    "# question answering format\n",
    "dataset['query'] = dataset.apply(\n",
    "    lambda row: row.subject + ' ' + row.predicate, axis=1)\n",
    "# adjust format in list \n",
    "# dataset['query'] = [dataset['query'][i].split()[:2] for i in range(100)]\n",
    "# dataset['query'] = dataset['query'].apply(lambda x: ' '.join(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>predicate</th>\n",
       "      <th>object</th>\n",
       "      <th>query</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>Aristotle</td>\n",
       "      <td>influences</td>\n",
       "      <td>Ayn_Rand</td>\n",
       "      <td>Aristotle influences</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>Aristotle</td>\n",
       "      <td>influences</td>\n",
       "      <td>Alexander_the_Great</td>\n",
       "      <td>Aristotle influences</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>Aristotle</td>\n",
       "      <td>influences</td>\n",
       "      <td>Avicenna</td>\n",
       "      <td>Aristotle influences</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>Aristotle</td>\n",
       "      <td>influences</td>\n",
       "      <td>Albertus_Magnus</td>\n",
       "      <td>Aristotle influences</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>Aristotle</td>\n",
       "      <td>influences</td>\n",
       "      <td>Augustine_of_Hippo</td>\n",
       "      <td>Aristotle influences</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>552</th>\n",
       "      <td>Angola</td>\n",
       "      <td>participatedIn</td>\n",
       "      <td>Battle_of_Cuito_Cuanavale</td>\n",
       "      <td>Angola participatedIn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>553</th>\n",
       "      <td>Angola</td>\n",
       "      <td>participatedIn</td>\n",
       "      <td>South_African_Border_War</td>\n",
       "      <td>Angola participatedIn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>554</th>\n",
       "      <td>Angola</td>\n",
       "      <td>exports</td>\n",
       "      <td>wordnet_lumber_114943580</td>\n",
       "      <td>Angola exports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555</th>\n",
       "      <td>Angola</td>\n",
       "      <td>participatedIn</td>\n",
       "      <td>Battle_of_Kitombo</td>\n",
       "      <td>Angola participatedIn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>556</th>\n",
       "      <td>Angola</td>\n",
       "      <td>participatedIn</td>\n",
       "      <td>Battle_of_Mbidizi_River</td>\n",
       "      <td>Angola participatedIn</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>296 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       subject       predicate                     object  \\\n",
       "261  Aristotle      influences                   Ayn_Rand   \n",
       "262  Aristotle      influences        Alexander_the_Great   \n",
       "263  Aristotle      influences                   Avicenna   \n",
       "264  Aristotle      influences            Albertus_Magnus   \n",
       "265  Aristotle      influences         Augustine_of_Hippo   \n",
       "..         ...             ...                        ...   \n",
       "552     Angola  participatedIn  Battle_of_Cuito_Cuanavale   \n",
       "553     Angola  participatedIn   South_African_Border_War   \n",
       "554     Angola         exports   wordnet_lumber_114943580   \n",
       "555     Angola  participatedIn          Battle_of_Kitombo   \n",
       "556     Angola  participatedIn    Battle_of_Mbidizi_River   \n",
       "\n",
       "                     query  \n",
       "261   Aristotle influences  \n",
       "262   Aristotle influences  \n",
       "263   Aristotle influences  \n",
       "264   Aristotle influences  \n",
       "265   Aristotle influences  \n",
       "..                     ...  \n",
       "552  Angola participatedIn  \n",
       "553  Angola participatedIn  \n",
       "554         Angola exports  \n",
       "555  Angola participatedIn  \n",
       "556  Angola participatedIn  \n",
       "\n",
       "[296 rows x 4 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"query\"], padding=\"max_length\", truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>predicate</th>\n",
       "      <th>object</th>\n",
       "      <th>query</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>Aristotle</td>\n",
       "      <td>influences</td>\n",
       "      <td>[input_ids, token_type_ids, attention_mask]</td>\n",
       "      <td>[input_ids, token_type_ids, attention_mask]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>Aristotle</td>\n",
       "      <td>influences</td>\n",
       "      <td>[input_ids, token_type_ids, attention_mask]</td>\n",
       "      <td>[input_ids, token_type_ids, attention_mask]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>Aristotle</td>\n",
       "      <td>influences</td>\n",
       "      <td>[input_ids, token_type_ids, attention_mask]</td>\n",
       "      <td>[input_ids, token_type_ids, attention_mask]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>Aristotle</td>\n",
       "      <td>influences</td>\n",
       "      <td>[input_ids, token_type_ids, attention_mask]</td>\n",
       "      <td>[input_ids, token_type_ids, attention_mask]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>Aristotle</td>\n",
       "      <td>influences</td>\n",
       "      <td>[input_ids, token_type_ids, attention_mask]</td>\n",
       "      <td>[input_ids, token_type_ids, attention_mask]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>552</th>\n",
       "      <td>Angola</td>\n",
       "      <td>participatedIn</td>\n",
       "      <td>[input_ids, token_type_ids, attention_mask]</td>\n",
       "      <td>[input_ids, token_type_ids, attention_mask]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>553</th>\n",
       "      <td>Angola</td>\n",
       "      <td>participatedIn</td>\n",
       "      <td>[input_ids, token_type_ids, attention_mask]</td>\n",
       "      <td>[input_ids, token_type_ids, attention_mask]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>554</th>\n",
       "      <td>Angola</td>\n",
       "      <td>exports</td>\n",
       "      <td>[input_ids, token_type_ids, attention_mask]</td>\n",
       "      <td>[input_ids, token_type_ids, attention_mask]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555</th>\n",
       "      <td>Angola</td>\n",
       "      <td>participatedIn</td>\n",
       "      <td>[input_ids, token_type_ids, attention_mask]</td>\n",
       "      <td>[input_ids, token_type_ids, attention_mask]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>556</th>\n",
       "      <td>Angola</td>\n",
       "      <td>participatedIn</td>\n",
       "      <td>[input_ids, token_type_ids, attention_mask]</td>\n",
       "      <td>[input_ids, token_type_ids, attention_mask]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>296 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       subject       predicate                                       object  \\\n",
       "261  Aristotle      influences  [input_ids, token_type_ids, attention_mask]   \n",
       "262  Aristotle      influences  [input_ids, token_type_ids, attention_mask]   \n",
       "263  Aristotle      influences  [input_ids, token_type_ids, attention_mask]   \n",
       "264  Aristotle      influences  [input_ids, token_type_ids, attention_mask]   \n",
       "265  Aristotle      influences  [input_ids, token_type_ids, attention_mask]   \n",
       "..         ...             ...                                          ...   \n",
       "552     Angola  participatedIn  [input_ids, token_type_ids, attention_mask]   \n",
       "553     Angola  participatedIn  [input_ids, token_type_ids, attention_mask]   \n",
       "554     Angola         exports  [input_ids, token_type_ids, attention_mask]   \n",
       "555     Angola  participatedIn  [input_ids, token_type_ids, attention_mask]   \n",
       "556     Angola  participatedIn  [input_ids, token_type_ids, attention_mask]   \n",
       "\n",
       "                                           query  \n",
       "261  [input_ids, token_type_ids, attention_mask]  \n",
       "262  [input_ids, token_type_ids, attention_mask]  \n",
       "263  [input_ids, token_type_ids, attention_mask]  \n",
       "264  [input_ids, token_type_ids, attention_mask]  \n",
       "265  [input_ids, token_type_ids, attention_mask]  \n",
       "..                                           ...  \n",
       "552  [input_ids, token_type_ids, attention_mask]  \n",
       "553  [input_ids, token_type_ids, attention_mask]  \n",
       "554  [input_ids, token_type_ids, attention_mask]  \n",
       "555  [input_ids, token_type_ids, attention_mask]  \n",
       "556  [input_ids, token_type_ids, attention_mask]  \n",
       "\n",
       "[296 rows x 4 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"query\"] = dataset[\"query\"].apply(lambda x: tokenizer(x, padding=\"max_length\", truncation=True))\n",
    "dataset[\"object\"] = dataset[\"object\"].apply(lambda x: tokenizer(x, padding=\"max_length\", truncation=True))\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_used = dataset.drop([\"subject\",\"predicate\"],axis =1)\n",
    "from sklearn.model_selection import train_test_split\n",
    "small_train_dataset, small_eval_dataset = train_test_split(dataset_used, shuffle = True, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForQuestionAnswering: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForQuestionAnswering\n",
    "\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\", evaluation_strategy=\"epoch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(model=model,args=training_args,train_dataset=small_train_dataset, \n",
    "                  eval_dataset=small_eval_dataset,compute_metrics=compute_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mwz_ryan\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\wz\\Desktop\\context-aware-embedding-master\\Resources\\code\\wandb\\run-20221027_130403-1y1c2ijs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/wz_ryan/huggingface/runs/1y1c2ijs\" target=\"_blank\">test_trainer</a></strong> to <a href=\"https://wandb.ai/wz_ryan/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyError",
     "evalue": "20",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3621\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3620\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3622\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx:136\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx:163\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5198\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5206\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 20",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[1;32mIn [18]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\trainer.py:925\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, **kwargs)\u001b[0m\n\u001b[0;32m    918\u001b[0m steps_in_epoch \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28mlen\u001b[39m(epoch_iterator)\n\u001b[0;32m    920\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m train_dataset_is_sized\n\u001b[0;32m    921\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mmax_steps \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n\u001b[0;32m    922\u001b[0m )\n\u001b[0;32m    923\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_epoch_begin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m--> 925\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(epoch_iterator):\n\u001b[0;32m    926\u001b[0m \n\u001b[0;32m    927\u001b[0m     \u001b[38;5;66;03m# Skip past any already trained steps if resuming training\u001b[39;00m\n\u001b[0;32m    928\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m steps_trained_in_current_epoch \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    929\u001b[0m         steps_trained_in_current_epoch \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:530\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    528\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    529\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[1;32m--> 530\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    531\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    532\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    533\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    534\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:570\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    568\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    569\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 570\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    571\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    572\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3505\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3503\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3504\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3505\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3507\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3623\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3622\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 3623\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3624\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3625\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3626\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3627\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3628\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 20"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import wandb\n",
    "import time\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from random import choices\n",
    "import matplotlib.pyplot as plt\n",
    "tqdm.pandas()\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import GPT2Tokenizer\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "from trl.gpt2 import GPT2HeadWithValueModel, respond_to_batch\n",
    "from trl.ppo import PPOTrainer\n",
    "from trl.core import build_bert_batch_from_txt\n",
    "\n",
    "config = {\n",
    "    \"lm_name\": \"lvwerra/gpt2-imdb\",\n",
    "    \"ref_lm_name\": \"lvwerra/gpt2-imdb\",\n",
    "    \"cls_model_name\": \"lvwerra/distilbert-imdb\",\n",
    "    \"tk_name\": \"gpt2\",\n",
    "    \"steps\": 320,\n",
    "    \"batch_size\": 32,\n",
    "    \"forward_batch_size\": 4,\n",
    "    \"ppo_epochs\": 4,   \n",
    "    \"txt_in_len\": 2,\n",
    "    \"txt_out_len\": 1,\n",
    "    \"lr\": 1.41e-5,\n",
    "    \"init_kl_coef\":0.2,\n",
    "    \"target\": 6,\n",
    "    \"horizon\":10000,\n",
    "    \"gamma\":1,\n",
    "    \"lam\":0.95,\n",
    "    \"cliprange\": .2,\n",
    "    \"cliprange_value\":.2,\n",
    "    \"vf_coef\":.1, \n",
    "    \"seed\": 1,\n",
    "}\n",
    "\n",
    "np.random.seed(config['seed'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1y1c2ijs) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd276d1da8464331b574ce634dd3a845",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.033 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.016705…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">test_trainer</strong>: <a href=\"https://wandb.ai/wz_ryan/huggingface/runs/1y1c2ijs\" target=\"_blank\">https://wandb.ai/wz_ryan/huggingface/runs/1y1c2ijs</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20221027_130403-1y1c2ijs\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1y1c2ijs). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "194ba0f592c24220884b78959c46961d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01693333333338766, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\wz\\Desktop\\context-aware-embedding-master\\Resources\\code\\wandb\\run-20221027_130511-31fhbjnn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/wz_ryan/Testing_largebatch/runs/31fhbjnn\" target=\"_blank\">trails #0</a></strong> to <a href=\"https://wandb.ai/wz_ryan/Testing_largebatch\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/wz_ryan/Testing_largebatch/runs/31fhbjnn?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x19f90cd5ca0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trails\n",
    "wandb.init(name='trails #0', project='Testing_largebatch', config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pre-trained GPT2 language models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2HeadWithValueModel were not initialized from the model checkpoint at lvwerra/gpt2-imdb and are newly initialized: ['transformer.h.0.attn.masked_bias', 'transformer.h.1.attn.masked_bias', 'transformer.h.2.attn.masked_bias', 'transformer.h.3.attn.masked_bias', 'transformer.h.4.attn.masked_bias', 'transformer.h.5.attn.masked_bias', 'transformer.h.6.attn.masked_bias', 'transformer.h.7.attn.masked_bias', 'transformer.h.8.attn.masked_bias', 'transformer.h.9.attn.masked_bias', 'transformer.h.10.attn.masked_bias', 'transformer.h.11.attn.masked_bias', 'v_head.summary.weight', 'v_head.summary.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of GPT2HeadWithValueModel were not initialized from the model checkpoint at lvwerra/gpt2-imdb and are newly initialized: ['transformer.h.0.attn.masked_bias', 'transformer.h.1.attn.masked_bias', 'transformer.h.2.attn.masked_bias', 'transformer.h.3.attn.masked_bias', 'transformer.h.4.attn.masked_bias', 'transformer.h.5.attn.masked_bias', 'transformer.h.6.attn.masked_bias', 'transformer.h.7.attn.masked_bias', 'transformer.h.8.attn.masked_bias', 'transformer.h.9.attn.masked_bias', 'transformer.h.10.attn.masked_bias', 'transformer.h.11.attn.masked_bias', 'v_head.summary.weight', 'v_head.summary.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "gpt2_model = GPT2HeadWithValueModel.from_pretrained(config['lm_name'])\n",
    "gpt2_model_ref = GPT2HeadWithValueModel.from_pretrained(config['ref_lm_name'])\n",
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained(config['tk_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = gpt2_model.to(device)\n",
    "_ = gpt2_model_ref.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.watch(gpt2_model, log='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load BERT classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\"\n",
    "\n",
    "sentiment_model = AutoModelForSequenceClassification.from_pretrained(config[\"cls_model_name\"])\n",
    "sentiment_tokenizer = AutoTokenizer.from_pretrained(config[\"cls_model_name\"])\n",
    "\n",
    "# model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.3350, -2.7266]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'this movie was really bad!!'\n",
    "output = sentiment_model.forward(sentiment_tokenizer.encode(text, return_tensors=\"pt\"))\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-2.7266, grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0][0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Token Prediction with GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Begin next-word using HF GPT-2 demo \n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "\n",
    "print(\"\\nBegin next-word using HF GPT-2 demo \")\n",
    "\n",
    "toker = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input sequence: \n",
      "connie own\n",
      "\n",
      "Tokenized input data structure: \n",
      "{'input_ids': tensor([[ 1102, 11952,   898]]), 'attention_mask': tensor([[1, 1, 1]])}\n",
      "\n",
      "Token IDs and their words: \n",
      "tensor(1102) con\n",
      "tensor(11952) nie\n",
      "tensor(898)  own\n",
      "\n",
      "All logits for next word: \n",
      "tensor([[-86.0371, -81.7703, -89.6548,  ..., -93.6331, -92.7520, -87.4399]])\n",
      "torch.Size([1, 50257])\n",
      "\n",
      "Predicted token ID of next word: \n",
      "262\n",
      "\n",
      "Predicted next word for sequence: \n",
      " the\n"
     ]
    }
   ],
   "source": [
    "seq = \"connie own\"\n",
    "print(\"\\nInput sequence: \")\n",
    "print(seq)\n",
    "\n",
    "inpts = toker(seq, return_tensors=\"pt\")\n",
    "print(\"\\nTokenized input data structure: \")\n",
    "print(inpts)\n",
    "\n",
    "inpt_ids = inpts[\"input_ids\"]  # just IDS, no attn mask\n",
    "print(\"\\nToken IDs and their words: \")\n",
    "for id in inpt_ids[0]:\n",
    "    word = toker.decode(id)\n",
    "    print(id, word)\n",
    "\n",
    "    \n",
    "with torch.no_grad():\n",
    "    logits = model(**inpts).logits[:, -1, :]\n",
    "\n",
    "print(\"\\nAll logits for next word: \")\n",
    "print(logits)\n",
    "print(logits.shape)\n",
    "\n",
    "pred_id = torch.argmax(logits).item()\n",
    "print(\"\\nPredicted token ID of next word: \")\n",
    "print(pred_id)\n",
    "\n",
    "pred_word = toker.decode(pred_id)\n",
    "print(\"\\nPredicted next word for sequence: \")\n",
    "print(pred_word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GPT_pred(seq):\n",
    "    inpts = toker(seq, return_tensors=\"pt\")\n",
    "    inpt_ids = inpts[\"input_ids\"]  # just IDS, no attn mask\n",
    "        \n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inpts).logits[:, -1, :]\n",
    "    pred_id = torch.argmax(logits).item()\n",
    "    pred_word = toker.decode(pred_id)\n",
    "    return logits, pred_id, pred_word\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_logit_to_reward(logit, task):\n",
    "    \"\"\"\n",
    "    Take the positive sentiment logit and scale it for the task.\n",
    "        task [negative]: reward = -logit\n",
    "        task [positive]: reward = logit\n",
    "    \"\"\"\n",
    "    for i in range(len(logit)):\n",
    "        if task[i]=='[false]':\n",
    "            logit[i] = -logit[i]\n",
    "        elif task[i]=='[true]':\n",
    "            pass\n",
    "        else:\n",
    "            raise ValueError('task has to be in [0, 1, 2]!')\n",
    "    return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wz\\AppData\\Local\\Temp\\ipykernel_25688\\1157424971.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['query'] = df.apply(\n",
      "C:\\Users\\wz\\AppData\\Local\\Temp\\ipykernel_25688\\1157424971.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['query'] = [df['query'][i].split()[:2] for i in range(1000)]\n",
      "C:\\Users\\wz\\AppData\\Local\\Temp\\ipykernel_25688\\1157424971.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['query'] = df['query'].apply(lambda x: ' '.join(x))\n",
      "C:\\Users\\wz\\AppData\\Local\\Temp\\ipykernel_25688\\1157424971.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['tokens'] = df['query'].apply(lambda x: gpt2_tokenizer.encode(' '+str(x), return_tensors=\"pt\").to(device))\n",
      "C:\\Users\\wz\\AppData\\Local\\Temp\\ipykernel_25688\\1157424971.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['tokens'] = df['tokens'].apply(lambda x: x[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20, 22, 37, 128, 146, 173, 190, 208, 219, 225, 232, 258, 478, 488, 531, 542, 577, 673, 684, 703, 714, 743, 753, 754, 777, 797, 838, 868, 891, 953, 963]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wz\\AppData\\Local\\Temp\\ipykernel_25688\\1157424971.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['tokens'] = df['tokens'].apply(lambda x: x[:2])\n"
     ]
    }
   ],
   "source": [
    "df = kg_df[:1000]\n",
    "\n",
    "# device = torch.device(\"cpu\")\n",
    "#model(img_test.unsqueeze(0).cuda()).detach().cpu().clone().numpy()\n",
    "\n",
    "# modify query as object + relation\n",
    "# e.g connie owns \n",
    "df['query'] = df.apply(\n",
    "    lambda row: row.source + ' ' + row.edge, axis=1)\n",
    "# adjust format in list \n",
    "df['query'] = [df['query'][i].split()[:2] for i in range(1000)]\n",
    "df['query'] = df['query'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# special token\n",
    "df['tokens'] = df['query'].apply(lambda x: gpt2_tokenizer.encode(' '+str(x), return_tensors=\"pt\").to(device))\n",
    "df['tokens'] = df['tokens'].apply(lambda x: x[0])\n",
    "df['tokens'] = df['tokens'].apply(lambda x: x[:2])\n",
    "\n",
    "# dropping invalid length in token column\n",
    "drop_row_index = []\n",
    "for i in range(1000):\n",
    "\n",
    "    if len(df['tokens'].loc[i]) < 2:\n",
    "        #df['tokens'].loc[i] = df['tokens'].loc[0].cuda().detach().cpu().clone().numpy()\n",
    "        drop_row_index.append(i)\n",
    "print(drop_row_index)\n",
    "df = df.drop(df.index[drop_row_index])\n",
    "        \n",
    "\n",
    "        \n",
    "# df.loc[(len(df['tokens']) < 2), 'tokens'] = df['tokens'].loc[0]\n",
    "#df['tokens'] = df['tokens'].apply(lambda x: df['tokens'].loc[0] if len(x) < 2)\n",
    "\n",
    "\n",
    "\n",
    "ctrl_str = ['[false]', '[true]']\n",
    "ctrl_tokens = dict((s, gpt2_tokenizer.encode(s, return_tensors=\"pt\").squeeze().to(device)) for s in ctrl_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "      <th>edge</th>\n",
       "      <th>query</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>examples</td>\n",
       "      <td>unfriended friend request</td>\n",
       "      <td>include</td>\n",
       "      <td>examples include</td>\n",
       "      <td>[tensor(6096, device='cuda:0'), tensor(2291, device='cuda:0')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>one</td>\n",
       "      <td>noticeable side effects</td>\n",
       "      <td>is sleeplessness</td>\n",
       "      <td>one is</td>\n",
       "      <td>[tensor(530, device='cuda:0'), tensor(318, device='cuda:0')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>early horror films</td>\n",
       "      <td>social  controversy</td>\n",
       "      <td>created great</td>\n",
       "      <td>early horror</td>\n",
       "      <td>[tensor(1903, device='cuda:0'), tensor(9961, device='cuda:0')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>however horror movies</td>\n",
       "      <td>positive  endings</td>\n",
       "      <td>present positive</td>\n",
       "      <td>however horror</td>\n",
       "      <td>[tensor(2158, device='cuda:0'), tensor(9961, device='cuda:0')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>satirical comedy drama</td>\n",
       "      <td>illicit love affair</td>\n",
       "      <td>is</td>\n",
       "      <td>satirical comedy</td>\n",
       "      <td>[tensor(40557, device='cuda:0'), tensor(10997, device='cuda:0')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>it</td>\n",
       "      <td>rated superhero site</td>\n",
       "      <td>is</td>\n",
       "      <td>it is</td>\n",
       "      <td>[tensor(340, device='cuda:0'), tensor(318, device='cuda:0')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>jenkins</td>\n",
       "      <td>1918 era london</td>\n",
       "      <td>make</td>\n",
       "      <td>jenkins make</td>\n",
       "      <td>[tensor(474, device='cuda:0'), tensor(268, device='cuda:0')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>gas</td>\n",
       "      <td>war</td>\n",
       "      <td>intended</td>\n",
       "      <td>gas intended</td>\n",
       "      <td>[tensor(3623, device='cuda:0'), tensor(5292, device='cuda:0')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>david</td>\n",
       "      <td>popular  mechanics</td>\n",
       "      <td>said</td>\n",
       "      <td>david said</td>\n",
       "      <td>[tensor(21970, device='cuda:0'), tensor(531, device='cuda:0')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>she</td>\n",
       "      <td>gender boundaries</td>\n",
       "      <td>has</td>\n",
       "      <td>she has</td>\n",
       "      <td>[tensor(673, device='cuda:0'), tensor(468, device='cuda:0')]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>969 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     source                     target              edge  \\\n",
       "0                  examples  unfriended friend request           include   \n",
       "1                       one    noticeable side effects  is sleeplessness   \n",
       "2        early horror films        social  controversy     created great   \n",
       "3     however horror movies          positive  endings  present positive   \n",
       "4    satirical comedy drama        illicit love affair                is   \n",
       "..                      ...                        ...               ...   \n",
       "995                      it       rated superhero site                is   \n",
       "996                 jenkins            1918 era london              make   \n",
       "997                     gas                        war          intended   \n",
       "998                   david         popular  mechanics              said   \n",
       "999                     she          gender boundaries               has   \n",
       "\n",
       "                query  \\\n",
       "0    examples include   \n",
       "1              one is   \n",
       "2        early horror   \n",
       "3      however horror   \n",
       "4    satirical comedy   \n",
       "..                ...   \n",
       "995             it is   \n",
       "996      jenkins make   \n",
       "997      gas intended   \n",
       "998        david said   \n",
       "999           she has   \n",
       "\n",
       "                                                               tokens  \n",
       "0      [tensor(6096, device='cuda:0'), tensor(2291, device='cuda:0')]  \n",
       "1        [tensor(530, device='cuda:0'), tensor(318, device='cuda:0')]  \n",
       "2      [tensor(1903, device='cuda:0'), tensor(9961, device='cuda:0')]  \n",
       "3      [tensor(2158, device='cuda:0'), tensor(9961, device='cuda:0')]  \n",
       "4    [tensor(40557, device='cuda:0'), tensor(10997, device='cuda:0')]  \n",
       "..                                                                ...  \n",
       "995      [tensor(340, device='cuda:0'), tensor(318, device='cuda:0')]  \n",
       "996      [tensor(474, device='cuda:0'), tensor(268, device='cuda:0')]  \n",
       "997    [tensor(3623, device='cuda:0'), tensor(5292, device='cuda:0')]  \n",
       "998    [tensor(21970, device='cuda:0'), tensor(531, device='cuda:0')]  \n",
       "999      [tensor(673, device='cuda:0'), tensor(468, device='cuda:0')]  \n",
       "\n",
       "[969 rows x 5 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PPO & Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6394267984578837,\n",
       " 0.025010755222666936,\n",
       " 1.2750293183691193,\n",
       " 0.22321073814882275]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# device = torch.device(\"cuda\")\n",
    "# device = \"cuda:0\"\n",
    "#input = input.to(device)\n",
    "#         model = model.to(device)\n",
    "\n",
    "a = [0,0,1,0]\n",
    "a = [i + random.random() for i in a ]\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "### KG check to modify reward value \n",
    "\n",
    "# given a range i.e. i*fbs to (i+1)*fbs\n",
    "# should output a tensor([ 0, 0,  1, 1], device='cuda:0')\n",
    "\n",
    "# pred is the reponse in batch_response\n",
    "# query is the query_list\n",
    "\n",
    "def KG_check(lower, upper):\n",
    "    tensor_array = []\n",
    "    for i in range(lower,upper):\n",
    "        # e.g. dark knight not\n",
    "#         print('lower is ',lower )\n",
    "#         print('upper is ',upper )\n",
    "#         print(\"i is \", i)\n",
    "#         print(len(query_list))\n",
    "#         print(len(batch_response))\n",
    "\n",
    "        # bugs here todo query_list[i].split() could be one element only\n",
    "        source = query_list[i].split()[0]\n",
    "        target = batch_response[i]\n",
    "        if len(query_list[i].split()) <2:\n",
    "            \n",
    "            relation = 'anything'\n",
    "        else:\n",
    "            relation = query_list[i].split()[1]\n",
    "        \n",
    "#         print(i)\n",
    "#         print(\"source is \",source)\n",
    "#         print(\"target is \",target)\n",
    "#         print(\"relation is \",relation)\n",
    "        if contains(source, target, relation):\n",
    "            tensor_array.append(1)\n",
    "        else:\n",
    "            tensor_array.append(0)\n",
    "            \n",
    "    # rand modular Todo: Fix this\n",
    "    c = random.randint(0, 3)\n",
    "    tensor_array[c]=1\n",
    "    # print(tensor_array)\n",
    "    \n",
    "    # add variant \n",
    "    tensor_array = [i + random.random() for i in tensor_array ]\n",
    "    return torch.cuda.LongTensor(tensor_array)\n",
    "\n",
    "# KG_check(28,32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"lm_name\": \"lvwerra/gpt2-imdb\",\n",
    "    \"ref_lm_name\": \"lvwerra/gpt2-imdb\",\n",
    "    \"cls_model_name\": \"lvwerra/distilbert-imdb\",\n",
    "    \"tk_name\": \"gpt2\",\n",
    "    \"steps\": 3200,\n",
    "    \"batch_size\": 32,\n",
    "    \"forward_batch_size\": 4,\n",
    "    \"ppo_epochs\": 4,   \n",
    "    \"txt_in_len\": 2,\n",
    "    \"txt_out_len\": 1,\n",
    "    \"lr\": 1.41e-5,\n",
    "    \"init_kl_coef\":0.2,\n",
    "    \"target\": 6,\n",
    "    \"horizon\":10000,\n",
    "    \"gamma\":1,\n",
    "    \"lam\":0.95,\n",
    "    \"cliprange\": .2,\n",
    "    \"cliprange_value\":.2,\n",
    "    \"vf_coef\":.1, \n",
    "    \"seed\": 1,\n",
    "}\n",
    "\n",
    "# where is lvwerra/distilbert-imdb?????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug\n",
    "torch.cuda.empty_cache()\n",
    "# df_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug\n",
    "\n",
    "# df_results = pd.DataFrame(game_data)\n",
    "# df_results\n",
    "#query_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                          | 0/100 [00:00<?, ?it/s]C:\\Users\\wz\\AppData\\Local\\Temp\\ipykernel_25688\\34421173.py:44: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  return torch.cuda.LongTensor(tensor_array)\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [10:34<00:00,  6.34s/it]\n"
     ]
    }
   ],
   "source": [
    "# for testing\n",
    "\n",
    "# batch_size: 32 -> df_batch\n",
    "# forward_batch_size: 4  -> fbs\n",
    "\n",
    "ppo_trainer = PPOTrainer(gpt2_model, gpt2_model_ref, **config)\n",
    "fbs = config['forward_batch_size']\n",
    "\n",
    "for epoch in tqdm(range(int(np.ceil(config[\"steps\"]/config['batch_size'])))):\n",
    "    torch.cuda.empty_cache()\n",
    "    logs = dict()\n",
    "    game_data = dict()\n",
    "    timing = dict()\n",
    "    t0 = time.time()\n",
    "    \n",
    "    #### get a batch from the dataset and annotate tasks\n",
    "    df_batch = df.sample(config['batch_size'])\n",
    "    task_list = choices(ctrl_str, k=config['batch_size'])\n",
    "    task_tensors = torch.stack([ctrl_tokens[t] for t in task_list])\n",
    "    query_list = df_batch['query'].tolist()\n",
    "    game_data['query'] = [t+q for t,q in zip(task_list, query_list)]\n",
    "    \n",
    "    \n",
    "    #print(task_tensors)\n",
    "    query_tensors = torch.stack((df_batch['tokens'].tolist()))\n",
    "    #print(\"before \",query_tensors)\n",
    "    query_tensors = torch.cat((task_tensors, query_tensors), axis=1)\n",
    "\n",
    "    \n",
    "    #### get response from gpt2\n",
    "    t = time.time()\n",
    "    response_tensors_array = []\n",
    "    for i in range(int(config['batch_size']/fbs)):\n",
    "        #print(int(config['batch_size']/fbs))\n",
    "        # 8 loops in total\n",
    "        # 0-4, 5-8, 9-12...\n",
    "        \n",
    "        # this is troublesome \n",
    "        # error: probability tensor contains either `inf`, `nan` or element < 0\n",
    "        #  next_token = torch.multinomial(probs, num_samples=1).squeeze(1) in gpt2.py function respond_to_batch\n",
    "        \n",
    "        # response  = respond_to_batch(gpt2_model, query_tensors[i*fbs:(i+1)*fbs],txt_len=config['txt_out_len'])\n",
    "        # we want a response of size [i*fbs:(i+1)*fbs]*1\n",
    "        response_tensors = []\n",
    "        for i in range(i*fbs, (i+1)*fbs):\n",
    "            seq = query_list[i]\n",
    "            inpts = toker(seq, return_tensors=\"pt\")\n",
    "            inpt_ids = inpts[\"input_ids\"]  # just IDS, no attn mask\n",
    "            for id in inpt_ids[0]:\n",
    "                word = toker.decode(id)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                logits = model(**inpts).logits[:, -1, :]\n",
    "            \n",
    "            \n",
    "            pred_id = torch.argmax(logits)\n",
    "        \n",
    "            response = pred_id\n",
    "            \n",
    "            # print(response.item())\n",
    "            response_tensors.append(response.item())\n",
    "        response_tensors = torch.cuda.LongTensor(response_tensors)\n",
    "        response_tensors_array.append(response_tensors)\n",
    "\n",
    "    # response_tensors.size = 32 * 1  \n",
    "    response_tensors = torch.cat(response_tensors_array,axis = -1 )\n",
    "    response_tensors  = torch.reshape(response_tensors, (-1, 1)) # force size of response_tensors.size = 32 * 1  rather that 32 length list\n",
    "    # print(response_tensors)\n",
    "    #print([gpt2_tokenizer.decode(response_tensors[i, :]) for i in range(config['batch_size'])])\n",
    "    batch_response = [gpt2_tokenizer.decode(response_tensors[i, :]) for i in range(config['batch_size'])] # list of length 32\n",
    "    game_data['response'] = batch_response\n",
    "    timing['time/get_response'] = time.time()-t\n",
    "    #print( game_data['response'])\n",
    "\n",
    "    #### tokenize text for sentiment analysis\n",
    "    t = time.time()\n",
    "    texts = [q + r for q,r in zip(query_list, game_data['response'])]\n",
    "    sentiment_inputs, attention_masks = build_bert_batch_from_txt(texts, sentiment_tokenizer, device)    \n",
    "    timing['time/build_input_sentiment'] = time.time()-t\n",
    "        \n",
    "    #### get sentiment score\n",
    "    t = time.time()\n",
    "    pos_logits = []\n",
    "    for i in range(int(config['batch_size']/fbs)):\n",
    "        input = {'input_ids':      sentiment_inputs[i*fbs:(i+1)*fbs],\n",
    "                  'attention_mask': attention_masks[i*fbs:(i+1)*fbs],\n",
    "                  'labels':         None\n",
    "                 }      \n",
    "        #res = sentiment_model.forward(**input)[0][:, 1].detach()\n",
    "        \n",
    "        device = torch.device(\"cuda\")\n",
    "        sentiment_model = sentiment_model.to(device)\n",
    "        res = sentiment_model.forward(sentiment_inputs[i*fbs:(i+1)*fbs],\n",
    "                                      attention_masks[i*fbs:(i+1)*fbs])[0][:, 1].detach()\n",
    "#         print(res)\n",
    "#         print(res.size())\n",
    "\n",
    "        # modify the rewards based on the KG\n",
    "#         print('lower is ',i*fbs )\n",
    "#         print('upper is ',(i+1)*fbs )\n",
    "        res = KG_check(i*fbs, (i+1)*fbs)\n",
    "\n",
    "        pos_logits.append(res)\n",
    "    \n",
    "    rewards = pos_logit_to_reward(torch.cat(pos_logits), task_list)\n",
    "    timing['time/get_sentiment_preds'] = time.time()-t\n",
    "\n",
    "\n",
    "    #### Run PPO training \n",
    "    t = time.time()\n",
    "    stats = ppo_trainer.step(query_tensors, response_tensors, rewards)\n",
    "    \n",
    "    # todo fix this \n",
    "    # enforce nan to zero for wandb to run\n",
    "    stats = np.nan_to_num(stats)\n",
    "    #print(stats)\n",
    "    timing['time/optimization'] = time.time()-t\n",
    "     \n",
    "    #### Log everything\n",
    "    timing['time/epoch'] = time.time()-t0\n",
    "    table_rows = [list(r) for r in zip(game_data['query'], game_data['response'], rewards.cpu().tolist())]\n",
    "    logs.update({'game_log':wandb.Table(\n",
    "        columns=['query', 'response', 'reward'],\n",
    "        rows=table_rows)})\n",
    "    logs.update(timing)\n",
    "    # comment to remove the nan issues\n",
    "    #logs.update(stats)\n",
    "    \n",
    "    # only support for float tensor\n",
    "    rewards = rewards.type(torch.FloatTensor)\n",
    "    \n",
    "    logs['env/reward_mean'] = torch.mean((rewards)).cpu().numpy()\n",
    "    logs['env/reward_std'] = torch.std(rewards).cpu().numpy()\n",
    "    logs['env/reward_dist'] = rewards.cpu().numpy()\n",
    "    for ctrl_s in ctrl_str:\n",
    "        key = 'env/reward_'+ctrl_s.strip('[]')\n",
    "        logs[key] = np.mean([r for r, t in zip(logs['env/reward_dist'], task_list) if t==ctrl_s])\n",
    "#     WandbCallback(log=None)\n",
    "    wandb.log(logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug\n",
    "\n",
    "\n",
    "#     cuda0 = torch.device('cuda:0')\n",
    "\n",
    "#     for i in range(config['batch_size']):\n",
    "#         current_row = df_batch['tokens'].iloc[i]\n",
    "    \n",
    "#         if type(current_row) is np.ndarray:\n",
    "#             #print('before ', current_row[0])\n",
    "#             #print(i)\n",
    "#             val = [current_row[0], current_row[1]]\n",
    "#             df_batch['tokens'].iloc[i] = list((torch.cuda.LongTensor([val[0]]),torch.cuda.LongTensor([val[1]])))\n",
    "#             print(i)\n",
    "#             print(df_batch['tokens'].iloc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i = [6096, 9000]\n",
    "# list((torch.cuda.LongTensor(i[0]),torch.cuda.LongTensor(i[1])))\n",
    "# #torch.cpu.as_tensor(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### get a batch from the dataset\n",
    "bs = 32\n",
    "game_data = dict()\n",
    "df_batch = df.sample(bs)\n",
    "query_list = df_batch['query'].tolist()\n",
    "game_data['query'] = query_list\n",
    "for ctrl in ctrl_str:\n",
    "    task_list = [ctrl] * bs\n",
    "    task_tensors = torch.stack([ctrl_tokens[t] for t in task_list])\n",
    "\n",
    "    query_tensors = torch.stack(df_batch['tokens'].tolist())\n",
    "    query_tensors = torch.cat((task_tensors, query_tensors), axis=1)\n",
    "\n",
    "    #### get response from gpt2 and gpt2_ref\n",
    "    response_tensors  = respond_to_batch(gpt2_model, query_tensors, txt_len=config['txt_out_len'])\n",
    "    game_data['response ' + ctrl] = [gpt2_tokenizer.decode(response_tensors[i, :]) for i in range(bs)]\n",
    "\n",
    "    #### sentiment analysis of query/response pairs before/after\n",
    "    texts = [q + r for q,r in zip(game_data['query'], game_data['response ' + ctrl])]\n",
    "    sentiment_inputs, attention_masks = build_bert_batch_from_txt(texts, sentiment_tokenizer, device)    \n",
    "    rewards = sentiment_model.forward(sentiment_inputs, attention_masks)[0][:, 1].detach()\n",
    "    game_data['rewards ' + ctrl] = pos_logit_to_reward(rewards, task_list).cpu().numpy()\n",
    "\n",
    "# store results in a dataframe\n",
    "df_results = pd.DataFrame(game_data)\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use BERT tokenizer\n",
    "\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "# call tokenize.encode function to convert sentences\n",
    "\n",
    "# # original sentence(X)\n",
    "# print(' Original sentence: ', X[0])\n",
    "\n",
    "# # tokenized sentence(X)\n",
    "# print('Tokenized sentence: ', bert_tokenizer.tokenize(X[0]))\n",
    "\n",
    "# # map sentence to token IDs\n",
    "# print('Mapping to IDs: ', bert_tokenizer.convert_tokens_to_ids(bert_tokenizer.tokenize(X[0])))\n",
    "\n",
    "# add [CLS] [SEP] [PAD] token as required \n",
    "# The encode funtion will 1. split into tokens 2. add [CLS] and [SEP] 3. map tokens to IDs\n",
    "input_id = []\n",
    "for s in X:\n",
    "    encoded  = bert_tokenizer.encode(s,  # the sentence for encoding\n",
    "                                     add_special_tokens = True,  # [CLS] and [SEP]\n",
    "                                     max_length = 200, # maximum length of sentence\n",
    "                                     pad_to_max_length = True, # padding length\n",
    "                                     return_tensors = 'pt' # return type is tensor\n",
    "                                    )\n",
    "    input_id.append(encoded)\n",
    "\n",
    "\n",
    "input_id = torch.cat(input_id, dim=0)\n",
    "label_y = torch.tensor(Y) \n",
    "\n",
    "# set epoch and batch_size\n",
    "\n",
    "epoch = 4\n",
    "batch_size = 32\n",
    "\n",
    "# split data for training and validation\n",
    "data = TensorDataset(input_id,label_y)\n",
    "data_len = len(data)\n",
    "train, validation = random_split(data, [int(0.8*data_len), data_len - int(0.8*data_len)])\n",
    "\n",
    "# create DataLoader for both training set and valiadation set\n",
    "train_dl = DataLoader(train, batch_size = batch_size, shuffle = True)\n",
    "val_dl = DataLoader(validation, batch_size = batch_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use BertForSequenceClassification as our model\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', \n",
    "                                                      num_labels=4, \n",
    "                                                      output_attentions=False, # no attention weight\n",
    "                                                      output_hidden_states=False # not need to return all hidden state )\n",
    "                                                     )\n",
    "# use cpu to train\n",
    "model.cpu()\n",
    "\n",
    "# optimizer and lr schedule\n",
    "\n",
    "# convention is to use AdamW where W is weight decay fix from huggingface\n",
    "optim = AdamW(model.parameters(), lr = 2e-5)\n",
    "scheduler = get_linear_schedule_with_warmup(optim, \n",
    "                                           num_warmup_steps = 0, # default\n",
    "                                           num_training_steps = len(train_dl)*epoch)\n",
    "\n",
    "# define accuracy by accuracy_score\n",
    "\n",
    "def accuracy(pred_label, y):\n",
    "    pred_label = np.argmax(pred_label, axis =1).flatten()\n",
    "    return accuracy_score(pred_label, y.flatten())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the seed value to make sure the return are determistic\n",
    "\n",
    "seed_value = 488\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "torch.manual_seed(seed_value)\n",
    "torch.cuda.manual_seed_all(seed_value)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "device = torch.device('cpu')\n",
    "\n",
    "\n",
    "# training phase\n",
    "\n",
    "for epc in range(epoch):\n",
    "    # accumulated loss for training and validation\n",
    "    training_loss = 0\n",
    "    validation_loss = 0\n",
    "    evaluation_acc = 0\n",
    "    \n",
    "    # training mode\n",
    "    model.train()\n",
    "    for step, batch in enumerate(train_dl):\n",
    "        model.zero_grad()\n",
    "        # each batch has 3 tensors: 1. input id 2. attention mask 3. label\n",
    "        # e.g. batch[i].to(device)\n",
    "        # follow the documentation on https://huggingface.co/transformers\n",
    "        input_id = batch[0].to(device)\n",
    "        att_mask = (batch[0]>0).to(device)\n",
    "        input_label = batch[1].to(device)\n",
    "        output = model(input_id, token_type_ids=None, attention_mask=att_mask, labels=input_label)\n",
    "        loss = output[0]\n",
    "        print(\"training batch \",step,\" with the loss of \", loss.item())\n",
    "        training_loss += loss.item()\n",
    "        # backprop\n",
    "        loss.backward()\n",
    "        # clip the gradient to 1.0, prevent exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        # update parameter\n",
    "        optim.step() \n",
    "        # update lr\n",
    "        scheduler.step()\n",
    "        \n",
    "    # eval mode\n",
    "    model.eval()\n",
    "    for step, batch in enumerate(val_dl):\n",
    "        # indicate the model not to compute gradients !\n",
    "        with torch.no_grad():\n",
    "            input_id = batch[0].to(device)\n",
    "            att_mask = (batch[0]>0).to(device)\n",
    "            input_label = batch[1].to(device)\n",
    "            output = model(input_id, token_type_ids=None, attention_mask=att_mask, labels=input_label)\n",
    "            loss = output[0] \n",
    "            print(\"val batch \",step,\" with the loss of \", loss.item())\n",
    "            validation_loss += loss.item()\n",
    "            pred = output[1].detach().cpu().numpy()\n",
    "            ground_truth_label = batch[1].to('cpu').numpy()\n",
    "            evaluation_acc += accuracy(pred, ground_truth_label)\n",
    "            \n",
    "    print('Validation loss: ', validation_loss / len(val_dl))\n",
    "    print('Overall accuracy: ', evaluation_acc / len(val_dl))\n",
    "\n",
    "print('Training finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing\n",
    "# predicate test label\n",
    "\n",
    "# add [CLS] [SEP] [PAD] token as required\n",
    "input_id_test = []\n",
    "for s in Xt:\n",
    "    encoded  = bert_tokenizer.encode(s, add_special_tokens = True, max_length = 200, \n",
    "                                pad_to_max_length = True, return_tensors = 'pt')\n",
    "    input_id_test.append(encoded)\n",
    "    \n",
    "input_id_test = torch.cat(input_id_test, dim=0)\n",
    "test_data = TensorDataset(input_id_test)\n",
    "test_dl = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "pred_label = []\n",
    "for step, (batch,) in enumerate(test_dl):\n",
    "    if step % 10 == 0:\n",
    "        print(\"processed with batch \", step)\n",
    "    with torch.no_grad():\n",
    "        output = model(batch.to(device), token_type_ids=None, attention_mask=(batch>0).to(device))\n",
    "        pred = output[0].detach().cpu().numpy()\n",
    "        pred_label.append(pred)\n",
    "print('Test predicting finished')\n",
    "final_preds = np.concatenate(pred_label, axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert probabilites into labels\n",
    "Y_test_pred = final_preds\n",
    "\n",
    "Y_pred = np.zeros((Y_test_pred.shape[0],1))\n",
    "\n",
    "for i in range (Y_test_pred.shape[0]):\n",
    "    array = Y_test_pred[i]\n",
    "    \n",
    "    max_val = np.max(array)\n",
    "    \n",
    "    index_max = np.where(array == max_val)\n",
    "    Y_pred[i] = index_max[0][0]\n",
    "    \n",
    "    \n",
    "print(Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write out the csv file\n",
    "# first column is the id, it is the index to the list of test examples\n",
    "# second column is the predction as an integer\n",
    "fout = open(\"out.csv\", \"w\")\n",
    "fout.write(\"Id,Y\\n\")\n",
    "for i, line in enumerate(Y_pred): # Y_test_pred is in the same order as the test data\n",
    "    fout.write(\"%d,%d\\n\" % (i, line))\n",
    "fout.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### REFERENCE\n",
    "\n",
    "# https://www.kaggle.com/hassanamin/bert-pytorch-cola-classification\n",
    "# https://www.cnblogs.com/tangjianwei/p/13334327.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
