{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cross-assist",
   "metadata": {},
   "source": [
    "# Fine-tune GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "regular-grace",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset \n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "from transformers import GPT2LMHeadModel, GPT2Config\n",
    "import numpy as np\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "successful-front",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in '/Users/wz/IdeaProjects/context-aware-embedding/Resources/code': ['Prototype 0.0.ipynb', 'Messing around.ipynb', '.DS_Store', 'Prototype0.1.ipynb', 'input', 'yago2', 'Q&A Baseline.ipynb', '.ipynb_checkpoints', 'accuracy.py', 'wandb']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hokkaido was formerly known as Ezo  Yezo  Yeso  or Yesso.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>According to Matsuura  the name was thought up...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In contrast to the island of Honshu  Hokkaido ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>From the Middle Ages  the people in Hokkaido b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hokkaido subsequently became known as Ezochi  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The disputes eventually developed into war.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40383</th>\n",
       "      <td>1494 Hosokawa Masamoto becomes Kyoto kanrei.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40384</th>\n",
       "      <td>1545 Hōjō Ujiyasu defeats the Uesugi clan forc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40385</th>\n",
       "      <td>1551 Mōri defeats the Ōuchi led by Sue Harukat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40386</th>\n",
       "      <td>1554 Mōri succeeds to Ōuchi lands and power.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40387</th>\n",
       "      <td>ISBN 978 0520208773.CS1 maint  ref harv  link ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40388 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Hokkaido was formerly known as Ezo  Yezo  Yeso  or Yesso.\n",
       "0      According to Matsuura  the name was thought up...       \n",
       "1      In contrast to the island of Honshu  Hokkaido ...       \n",
       "2      From the Middle Ages  the people in Hokkaido b...       \n",
       "3      Hokkaido subsequently became known as Ezochi  ...       \n",
       "4            The disputes eventually developed into war.       \n",
       "...                                                  ...       \n",
       "40383       1494 Hosokawa Masamoto becomes Kyoto kanrei.       \n",
       "40384  1545 Hōjō Ujiyasu defeats the Uesugi clan forc...       \n",
       "40385  1551 Mōri defeats the Ōuchi led by Sue Harukat...       \n",
       "40386       1554 Mōri succeeds to Ōuchi lands and power.       \n",
       "40387  ISBN 978 0520208773.CS1 maint  ref harv  link ...       \n",
       "\n",
       "[40388 rows x 1 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "# import wikipedia sentences\n",
    "import os\n",
    "\n",
    "cwd = os.getcwd()  # Get the current working directory (cwd)\n",
    "files = os.listdir(cwd)  # Get all the files in that directory\n",
    "print(\"Files in %r: %s\" % (cwd, files))\n",
    "\n",
    "req = pd.read_fwf(\"/Users/wz/IdeaProjects/context-aware-embedding/Resources/code/input/japan_wiki.txt\")\n",
    "req\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7c886a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "master = \"https://raw.githubusercontent.com/mcelikkaya/medium_articles/main/japan_wiki.txt\"\n",
    "req = requests.get(master)\n",
    "req = req.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48a7a605",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sentences = req.split(\"\\n\")\n",
    "all_sentences = [s.replace(\"\\r\",\"\") for s in all_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bridal-scanner",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample size :  40389\n",
      "samples     : \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Hokkaido was formerly known as Ezo  Yezo  Yeso  or Yesso.',\n",
       " 'According to Matsuura  the name was thought up because the Ainu called the region Kai.',\n",
       " 'In contrast to the island of Honshu  Hokkaido saw an absence of conflict during this time period.',\n",
       " 'From the Middle Ages  the people in Hokkaido began to be called Ezo.',\n",
       " 'Hokkaido subsequently became known as Ezochi  蝦夷地  lit.',\n",
       " 'The disputes eventually developed into war.',\n",
       " 'Takeda Nobuhiro killed the Ainu leader  Koshamain  and defeated the opposition in 1457.',\n",
       " 'The Matsumae family s economy relied upon trade with the Ainu.',\n",
       " 'They held authority over the south of Ezochi until the end of the Edo period.',\n",
       " 'There were numerous revolts by the Ainu against the feudal rule.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"sample size : \",len(all_sentences))\n",
    "print(\"samples     : \" )\n",
    "all_sentences[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "brown-public",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16504, 11790]\n",
      "[16504]\n",
      "[73, 2674, 284, 2584, 78]\n",
      "[73, 2674]\n",
      "[83, 482, 8226]\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "#get pretrained tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2', bos_token='<sos>', eos_token='<eos>', pad_token='<pad>')\n",
    "\n",
    "#tokenizer some samples\n",
    "print( tokenizer.encode(\"Japan Tokyo\") )\n",
    "print( tokenizer.encode(\"Japan\") )\n",
    "print( tokenizer.encode(\"japan tokyo\") )\n",
    "print( tokenizer.encode(\"japan\") )\n",
    "print( tokenizer.encode(\"tokyo\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "gentle-estate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_len 85\n"
     ]
    }
   ],
   "source": [
    "max_len = max([len(tokenizer.encode(s)) for s in all_sentences])\n",
    "\n",
    "print(f\"max_len {max_len}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "small-platform",
   "metadata": {},
   "outputs": [],
   "source": [
    "#since we will be feeding with sentences from wikipedia\n",
    "#we can mark beginning and end of sentences with with sos and eos\n",
    "def tokenize_seq(sent,tokenizer,max_length):\n",
    "    return tokenizer('<sos>'+ sent + '<eos>', truncation=True, max_length=max_length, padding=\"max_length\")\n",
    "\n",
    "class JapanDataset(Dataset):\n",
    "\n",
    "    def __init__(self, sentences, tokenizer, gpt2_type=\"gpt2\", max_length=max_len):\n",
    "\n",
    "        self.tokenizer = tokenizer \n",
    "        self.input_ids = []\n",
    "        self.attn_masks = []\n",
    "\n",
    "        for sentence in sentences:      \n",
    "            encodings = tokenize_seq(sentence,tokenizer,max_length)\n",
    "\n",
    "            self.input_ids.append(torch.tensor(encodings['input_ids']))\n",
    "            self.attn_masks.append(torch.tensor(encodings['attention_mask']))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.attn_masks[idx]   \n",
    "\n",
    "def format_time(elapsed):\n",
    "    return str(datetime.timedelta(seconds=int(round((elapsed)))))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "satisfactory-dictionary",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "saving-configuration",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_size : 36350\n",
      "val_size   : 4039\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create an instance of Dataset\n",
    "dataset = JapanDataset(all_sentences, tokenizer, max_length=max_len)\n",
    "\n",
    "# Split into training and validation sets\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "train_set, val_set = random_split(dataset, [train_size, val_size])\n",
    "print(\"train_size :\",train_size)\n",
    "print(\"val_size   :\",val_size)\n",
    "\n",
    "gc.collect() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "sticky-christian",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([50257,    39,   482,    74, 44354,   373, 15734,  1900,   355,   412,\n",
       "         10872,   220,   575,  8471,    78,   220,  3363,    78,   220,   393,\n",
       "           575,   408,    78,    13, 50258, 50259, 50259, 50259, 50259, 50259,\n",
       "         50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259,\n",
       "         50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259,\n",
       "         50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259,\n",
       "         50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259,\n",
       "         50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259,\n",
       "         50259, 50259, 50259, 50259, 50259]),\n",
       " tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lets check a sample from dataset \n",
    "#50257 beginning of sentence token\n",
    "#50258 end of sentence token\n",
    "#50259 pad token\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "corresponding-stage",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define dataloaders\n",
    "train_dataloader = DataLoader(train_set,  sampler = RandomSampler(train_set), batch_size = 32)\n",
    "validation_dataloader = DataLoader(val_set, sampler = SequentialSampler(val_set), batch_size = 32 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "timely-conversation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create default config\n",
    "configuration = GPT2Config.from_pretrained('gpt2', output_hidden_states=False)\n",
    "# Load pretrained gpt2\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\", config=configuration)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Create device\n",
    "device = torch.device(\"cpu\")\n",
    "model.cpu()\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr = 0.0005)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "rocky-result",
   "metadata": {},
   "outputs": [],
   "source": [
    "#at every step i want to check if generations are getting better.\n",
    "def eval_keywords(keywords):\n",
    "    model.eval()\n",
    "    for keyword in keywords:\n",
    "        input_seq = \"<sos> \" + keyword\n",
    "        generated = torch.tensor(tokenizer.encode(input_seq)).unsqueeze(0)\n",
    "        generated = generated.to(device)\n",
    "        sample_outputs = model.generate(\n",
    "                                generated, \n",
    "                                do_sample=True,   \n",
    "                                top_k=30, \n",
    "                                max_length = 50,\n",
    "                                top_p=0.90, \n",
    "                                num_return_sequences=2\n",
    "                                )\n",
    "    for i, sample_output in enumerate(sample_outputs):\n",
    "        print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))\n",
    "\n",
    "keywords = [\"Osaka\",\"Japan\",\"Kyoto\",\"Yokohama\",\"Kanto\",\"Nikko\",\"Japan has\",\"Tokyo is the\",\"Osaka is the\",\"Kyoto is the\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "white-softball",
   "metadata": {},
   "outputs": [],
   "source": [
    "#call model with a batch of input\n",
    "def process_one_batch(batch):\n",
    "    b_input_ids = batch[0].to(device)\n",
    "    b_labels = batch[0].to(device)\n",
    "    b_masks = batch[1].to(device)\n",
    "    outputs  = model(b_input_ids,  attention_mask = b_masks,labels=b_labels)\n",
    "    return outputs\n",
    "\n",
    "#do one epoch for training\n",
    "def train_epoch():\n",
    "    t0 = time.time()\n",
    "    total_train_loss = 0\n",
    "    model.train()\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        \n",
    "        model.zero_grad()        \n",
    "        outputs = process_one_batch( batch)\n",
    "        loss = outputs[0]  \n",
    "        batch_loss = loss.item()\n",
    "        total_train_loss += batch_loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        \n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)  \n",
    "    print(\"avg_train_loss\",avg_train_loss)  \n",
    "    elapsed_time = format_time(time.time() - t0)\n",
    "    print(\"elapsed time for 1 training epoch : \",elapsed_time)\n",
    "\n",
    "#do one epoch for eval\n",
    "def eval_epoch():\n",
    "    t0 = time.time()\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in validation_dataloader:            \n",
    "        \n",
    "        with torch.no_grad():        \n",
    "            outputs = process_one_batch( batch)\n",
    "            loss = outputs[0]              \n",
    "            batch_loss = loss.item()\n",
    "            total_eval_loss += batch_loss         \n",
    "\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    print(\"avg_val_loss\",avg_val_loss) \n",
    "    elapsed_time = format_time(time.time() - t0)\n",
    "    print(\"elapsed time for 1 eval epoch : \",elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "mineral-roman",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/g6/dmtl65514z1f6797p1f74l1w0000gn/T/ipykernel_17513/1146828650.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#train eval 1 cycle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#then create sample sentences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0meval_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0meval_keywords\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mkeywords\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/g6/dmtl65514z1f6797p1f74l1w0000gn/T/ipykernel_17513/377436546.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mtotal_train_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#train eval 1 cycle\n",
    "#then create sample sentences\n",
    "train_epoch()\n",
    "eval_epoch()\n",
    "eval_keywords( keywords )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a510d0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
