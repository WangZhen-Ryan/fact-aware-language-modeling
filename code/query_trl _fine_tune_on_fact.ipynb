{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f8d3b6f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import random\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "import nltk\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "import bs4\n",
    "import requests\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "from spacy.matcher import Matcher \n",
    "from spacy.tokens import Span \n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# import relevant LMs package\n",
    "\n",
    "import transformers\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "from transformers import BertForSequenceClassification, AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from transformers import GPT2Model, GPT2Tokenizer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# cheak to use gpu or cpu\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Use gpu\")\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    print(\"Use cpu\")\n",
    "    device = torch.device(\"cpu\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "device = \"cuda:0\"\n",
    "torch.cuda.empty_cache()\n",
    "# torch.cuda.reset_max_memory_allocated()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "path='C:\\\\Users\\\\wz\\\\Desktop\\\\DBpedia_query\\\\train\\\\DBpedia_query_train.csv'\n",
    "DBpedia = pd.read_csv(path)\n",
    "DBpedia"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# next token prediction\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "\n",
    "print(\"\\nBegin next-word using HF GPT-2 demo \")\n",
    "\n",
    "toker = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from  transformers import GPT2TokenizerFast, GPT2LMHeadModel\n",
    "\n",
    "base_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "vocabulary = base_tokenizer.get_vocab()\n",
    "vocabulary['University']\n",
    "\n",
    "text = 'University of Wisconsin%E2%80%93Madison'\n",
    "sub = 'University'\n",
    "text_ids = base_tokenizer.encode(sub, return_tensors = 'pt')\n",
    "text_ids.item()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "# from  transformers import GPT2TokenizerFast, GPT2LMHeadModel\n",
    "# from torch.nn import CrossEntropyLoss\n",
    "\n",
    "# model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "# model.eval()\n",
    "# tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "\n",
    "# ### We calculate the hidden_states and the past of the common left part of the sentence\n",
    "# past = \"I like sitting in my new chair and\"\n",
    "# past_tokenize_input = tokenizer.tokenize(past)\n",
    "# past_tensor_input = torch.tensor([tokenizer.convert_tokens_to_ids(past_tokenize_input)])\n",
    "\n",
    "# past_last_hidden_state, past = model.transformer(past_tensor_input)\n",
    "\n",
    "# print(past_last_hidden_state)\n",
    "\n",
    "# def score(sentence, past, past_last_hidden_state, past_tensor_input):\n",
    "#     tokenize_input = tokenizer.tokenize(sentence, )\n",
    "#     tensor_input = torch.tensor([tokenizer.convert_tokens_to_ids(tokenize_input)])\n",
    "\n",
    "#     ###the following code is slightly modified from https://github.com/huggingface/transformers/blob/09a2f40684f77e62d0fd8485fe9d2d610390453f/src/transformers/modeling_gpt2.py#L604\n",
    "#     ###now we calculate the right part of the sentence with the already calculated past\n",
    "#     transformer_outputs = model.transformer(\n",
    "#             tensor_input,\n",
    "#             attention_mask=None,\n",
    "#             token_type_ids=None,\n",
    "#             position_ids=None,\n",
    "#             head_mask=None,\n",
    "#             inputs_embeds=None,\n",
    "#             use_cache=None,\n",
    "#             output_attentions=None,\n",
    "#             output_hidden_states=None,\n",
    "#         )\n",
    "    \n",
    "#     # print(past_last_hidden_state)\n",
    "        \n",
    "#     ###and concatenate the output of with the hidden_state of the left part of the sentence\n",
    "#     hidden_states = torch.cat((past_last_hidden_state, transformer_outputs[0]), dim=1)\n",
    "    \n",
    "#     ###the following part is exactly the same as https://github.com/huggingface/transformers/blob/09a2f40684f77e62d0fd8485fe9d2d610390453f/src/transformers/modeling_gpt2.py#L604\n",
    "#     lm_logits = model.lm_head(hidden_states)\n",
    "\n",
    "#     labels_input = torch.cat((past_tensor_input, tensor_input), dim=1)\n",
    "\n",
    "#     # Shift so that tokens < n predict n\n",
    "#     shift_logits = lm_logits[..., :-1, :].contiguous()\n",
    "#     shift_labels = labels_input[..., 1:].contiguous()\n",
    "#     # Flatten the tokens\n",
    "#     loss_fct = CrossEntropyLoss()\n",
    "#     loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "#     return -loss.item()\n",
    "\n",
    "# candidates = [\"watch\", \"run\", \"think\", \"apple\", \"light\"]\n",
    "\n",
    "# sent_template = \" {} about life\"\n",
    "\n",
    "# print({candidate: score((sent_template.format(candidate)), past, past_last_hidden_state, past_tensor_input) for candidate in candidates})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# get a subgraph that include the true answer with input row\n",
    "def get_label(row, n):\n",
    "    text = DBpedia.loc[3,\"query\"]\n",
    "    true_answer = DBpedia.loc[3,\"response\"]\n",
    "    DBpedia_s = DBpedia.sample(n-1)\n",
    "    DBpedia_row = DBpedia.iloc[[row]]\n",
    "    DBpedia_sub = pd.concat([DBpedia_s,DBpedia_row], axis=0)\n",
    "    # get labels\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    all_response = DBpedia_sub.loc[:,'response'].values\n",
    "    le.fit(all_response)\n",
    "    label_value = le.transform(all_response)\n",
    "    # form a dict labels\n",
    "    labels = dict()\n",
    "    for i in range(len(all_response)):\n",
    "        labels[all_response[i]] = label_value[i]\n",
    "    # print(len(all_response))\n",
    "    return text, true_answer, all_response, label_value, labels"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# all_response = all_response[0:100]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "model.eval()\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "def next_word_prediction(sentence):\n",
    "    tokenize_input = tokenizer.tokenize(sentence)\n",
    "    tensor_input = torch.tensor([tokenizer.convert_tokens_to_ids(tokenize_input)])\n",
    "    loss = model(tensor_input, labels=tensor_input)\n",
    "    return -loss[0].item()\n",
    "\n",
    "def outcome(candidates, sentence, true_answer):\n",
    "    # candidates = all_response\n",
    "    sent_template = sentence + \" {}\"\n",
    "    # print({candidate: next_word_prediction(sent_template.format(candidate)) for candidate in candidates})\n",
    "    response_prob = {candidate: next_word_prediction(sent_template.format(candidate)) for candidate in candidates}\n",
    "    max_key = max(response_prob, key=lambda k: response_prob[k])\n",
    "    # loss from true response \n",
    "    loss = response_prob[max_key] - response_prob[true_answer]\n",
    "    return max_key, loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def next_word_GPT2(row, n):\n",
    "    # print( get_label(3,100))\n",
    "    text, true_answer, all_response, label_value, labels = get_label(3,100)\n",
    "    max_key, loss = outcome(all_response,text,true_answer)\n",
    "    return max_key, loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "next_word_GPT2(10,100)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # get a bound for loss for 3 trail\n",
    "# for it in range(3):\n",
    "#     array = []\n",
    "#     for i in tqdm(range(len(DBpedia))):\n",
    "#         array.append(next_word_GPT2(i,100))\n",
    "#     print(\"The median is \", np.median(array))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# np.median(array)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import wandb\n",
    "import time\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from random import choices\n",
    "import matplotlib.pyplot as plt\n",
    "tqdm.pandas()\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import GPT2Tokenizer\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "from trl.gpt2 import GPT2HeadWithValueModel, respond_to_batch\n",
    "from trl.ppo import PPOTrainer\n",
    "from trl.core import build_bert_batch_from_txt\n",
    "\n",
    "config = {\n",
    "    \"lm_name\": \"lvwerra/gpt2\",\n",
    "    \"ref_lm_name\": \"lvwerra/gpt2\",\n",
    "    \"tk_name\": \"gpt2\",\n",
    "    \"steps\": 6400,\n",
    "    \"batch_size\": 64,\n",
    "    \"forward_batch_size\": 8,\n",
    "    \"ppo_epochs\": 4,   \n",
    "    \"txt_in_len\": 5,\n",
    "    \"txt_out_len\": 15,\n",
    "    \"lr\": 1.41e-5,\n",
    "    \"init_kl_coef\":0.2,\n",
    "    \"target\": 6,\n",
    "    \"horizon\":10000,\n",
    "    \"gamma\":1,\n",
    "    \"lam\":0.95,\n",
    "    \"cliprange\": .2,\n",
    "    \"cliprange_value\":.2,\n",
    "    \"vf_coef\":.1, \n",
    "    \"seed\": 1,\n",
    "}\n",
    "\n",
    "np.random.seed(config['seed'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# trails\n",
    "wandb.init(name='trails #7', project='gpt2-query', config=config)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# load models\n",
    "gpt2_model = GPT2HeadWithValueModel.from_pretrained(\"gpt2\")\n",
    "gpt2_model_ref = GPT2HeadWithValueModel.from_pretrained(\"gpt2\")\n",
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained(config['tk_name'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "_ = gpt2_model.to(device)\n",
    "_ = gpt2_model_ref.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "wandb.watch(gpt2_model, log='all')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# bert classifier\n",
    "# unused\n",
    "device = \"cuda:0\"\n",
    "sentiment_model = AutoModelForSequenceClassification.from_pretrained(\"lvwerra/distilbert-imdb\")\n",
    "sentiment_tokenizer = AutoTokenizer.from_pretrained(\"lvwerra/distilbert-imdb\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# rename\n",
    "df = DBpedia.copy()\n",
    "ctrl_str = ['[false]', '[true]']\n",
    "ctrl_tokens = dict((s, gpt2_tokenizer.encode(s, return_tensors=\"pt\").squeeze().to(device)) for s in ctrl_str)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# special token\n",
    "df['tokens'] = df['query'].apply(lambda x: gpt2_tokenizer.encode(' '+str(x), return_tensors=\"pt\").to(device))\n",
    "df['tokens'] = df['tokens'].apply(lambda x: x[0])\n",
    "df['tokens'] = df['tokens'].apply(lambda x: x[:2])\n",
    "\n",
    "# dropping invalid length in token column\n",
    "drop_row_index = []\n",
    "for i in range(1000):\n",
    "\n",
    "    if len(df['tokens'].loc[i]) < 2:\n",
    "        #df['tokens'].loc[i] = df['tokens'].loc[0].cuda().detach().cpu().clone().numpy()\n",
    "        drop_row_index.append(i)\n",
    "print(drop_row_index)\n",
    "df = df.drop(df.index[drop_row_index])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### load trained model from finetune on facts 0/1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class SimpleGPT2SequenceClassifier(nn.Module):\n",
    "    def __init__(self, hidden_size: int, num_classes:int ,max_seq_len:int, gpt_model_name:str):\n",
    "        super(SimpleGPT2SequenceClassifier,self).__init__()\n",
    "        self.gpt2model = GPT2Model.from_pretrained(gpt_model_name)\n",
    "        self.fc1 = nn.Linear(hidden_size*max_seq_len, num_classes)\n",
    "\n",
    "        \n",
    "    def forward(self, input_id, mask):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "                input_id: encoded inputs ids of sent.\n",
    "        \"\"\"\n",
    "        gpt_out, _ = self.gpt2model(input_ids=input_id, attention_mask=mask, return_dict=False)\n",
    "        batch_size = gpt_out.shape[0]\n",
    "        linear_output = self.fc1(gpt_out.view(batch_size,-1))\n",
    "        return linear_output"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_new = SimpleGPT2SequenceClassifier(hidden_size=768, num_classes=2, max_seq_len=10, gpt_model_name=\"gpt2\")\n",
    "model_new.load_state_dict(torch.load(\"C:\\\\Users\\\\wz\\\\Desktop\\\\Finetune on 0,1\\\\model\\\\gpt2-text-classifier-model.pt\"))\n",
    "model_new.eval()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### use the model we trained on fact "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "example_text = \"\"\"\n",
    "Fairview, California country Iran\n",
    "\"\"\"\n",
    "def fact_eval(text):\n",
    "    fixed_text = \" \".join(example_text.lower().split())\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    tokenizer.padding_side = \"left\"\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    model_input = tokenizer(fixed_text, padding='max_length', max_length=10, truncation=True, return_tensors=\"pt\")\n",
    "    mask = model_input['attention_mask'].cpu()\n",
    "    input_id = model_input[\"input_ids\"].squeeze(1).cpu()\n",
    "\n",
    "    output = model_new(input_id, mask)\n",
    "    prob = torch.nn.functional.softmax(output, dim=1)[0]\n",
    "    pred_fact = output.argmax(dim=1).item()\n",
    "    # 0 for false, 1 for true\n",
    "    return pred_fact"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# fact_eval(example_text)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def pos_logit_to_reward(logit, task):\n",
    "    \"\"\"\n",
    "    Take the positive sentiment logit and scale it for the task.\n",
    "        task [negative]: reward = -logit\n",
    "        task [positive]: reward = logit\n",
    "    \"\"\"\n",
    "    for i in range(len(logit)):\n",
    "        if task[i]=='[false]':\n",
    "            logit[i] = -logit[i]\n",
    "        elif task[i]=='[true]':\n",
    "            pass\n",
    "        else:\n",
    "            raise ValueError('task has to be in [0, 1, 2]!')\n",
    "    return logit"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### KG check to modify reward value \n",
    "\n",
    "# given a range i.e. i*fbs to (i+1)*fbs\n",
    "# should output a tensor([ 0, 0,  1, 1], device='cuda:0')\n",
    "\n",
    "# pred is the reponse in batch_response\n",
    "# query is the query_list\n",
    "\n",
    "def KG_check(lower, upper):\n",
    "    factuals = []\n",
    "    for i in range(lower,upper):\n",
    "        query = DBpedia.loc[i,\"query\"]\n",
    "        response = DBpedia.loc[i,\"response\"]\n",
    "        text = \"\" + query + response\n",
    "        # print(text)\n",
    "        factual = fact_eval(text)\n",
    "        factuals.append(factual)\n",
    "    # add variant \n",
    "    tensor_array = [i + random.random() for i in factuals ]\n",
    "    return torch.cuda.LongTensor(tensor_array)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "KG_check(3, 7)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# batch_size: 32\n",
    "# forward_batch_size: 4 \n",
    "\n",
    "ppo_trainer = PPOTrainer(gpt2_model, gpt2_model_ref, **config)\n",
    "fbs = config['forward_batch_size']\n",
    "\n",
    "for epoch in tqdm(range(int(np.ceil(config[\"steps\"]/config['batch_size'])))):\n",
    "    torch.cuda.empty_cache()\n",
    "    logs = dict()\n",
    "    game_data = dict()\n",
    "    timing = dict()\n",
    "    t0 = time.time()\n",
    "    \n",
    "    #### get a batch from the dataset and annotate tasks\n",
    "    df_batch = df.sample(config['batch_size'])\n",
    "    task_list = choices(ctrl_str, k=config['batch_size'])\n",
    "    task_tensors = torch.stack([ctrl_tokens[t] for t in task_list])\n",
    "    query_list = df_batch['query'].tolist()\n",
    "    game_data['query'] = [t+q for t,q in zip(task_list, query_list)]\n",
    "    \n",
    "    \n",
    "    #print(task_tensors)\n",
    "    query_tensors = torch.stack((df_batch['tokens'].tolist()))\n",
    "    #print(\"before \",query_tensors)\n",
    "    query_tensors = torch.cat((task_tensors, query_tensors), axis=1)\n",
    "\n",
    "    \n",
    "    #### get response from gpt2\n",
    "    t = time.time()\n",
    "    total_length = config['txt_in_len']+config['txt_out_len']\n",
    "    response_tensors_array = []\n",
    "    for i in range(int(config['batch_size']/fbs)):\n",
    "        response  = respond_to_batch(gpt2_model, query_tensors[i*fbs:(i+1)*fbs],\n",
    "                                     txt_len=config['txt_out_len'])\n",
    "        response_tensors_array.append(response)\n",
    "    response_tensors = torch.cat(response_tensors_array)\n",
    "#         #print(int(config['batch_size']/fbs))\n",
    "#         # 8 loops in total\n",
    "#         # 0-4, 5-8, 9-12...\n",
    "        \n",
    "#         # this is troublesome \n",
    "#         # error: probability tensor contains either `inf`, `nan` or element < 0\n",
    "#         #  next_token = torch.multinomial(probs, num_samples=1).squeeze(1) in gpt2.py function respond_to_batch\n",
    "        \n",
    "#         # response  = respond_to_batch(gpt2_model, query_tensors[i*fbs:(i+1)*fbs],txt_len=config['txt_out_len'])\n",
    "#         # we want a response of size [i*fbs:(i+1)*fbs]*1\n",
    "#         response_tensors = []\n",
    "#         for i in range(i*fbs, (i+1)*fbs):\n",
    "            \n",
    "#             seq = query_list[i]\n",
    "#             inpts = toker(seq, return_tensors=\"pt\")\n",
    "#             inpt_ids = inpts[\"input_ids\"]  # just IDS, no attn mask\n",
    "            \n",
    "#             key, loss = next_word_GPT2(i,100)\n",
    "            \n",
    "#             pred_id = base_tokenizer.encode(key, return_tensors = 'pt')\n",
    "#             # now it force response list into one tensor\n",
    "#             response = (pred_id[0])[1] # todo change it  should be one id not a list\n",
    "#             # print(response)\n",
    "#             response_tensors.append(response.item())\n",
    "#         response_tensors = torch.cuda.LongTensor(response_tensors)\n",
    "#         response_tensors_array.append(response_tensors)\n",
    "   \n",
    "#     # response_tensors.size = 32 * 1  \n",
    "#     response_tensors = torch.cat(response_tensors_array,axis = -1 )\n",
    "#     response_tensors  = torch.reshape(response_tensors, (-1, 1)) # force size of response_tensors.size = 32 * 1  rather that 32 length list\n",
    "#     # print(response_tensors)\n",
    "#     # print([gpt2_tokenizer.decode(response_tensors[i, :]) for i in range(config['batch_size'])])\n",
    "#     # print(response_tensors)\n",
    "    batch_response = [gpt2_tokenizer.decode(response_tensors[i, :]) for i in range(config['batch_size'])] # list of length 32\n",
    "    game_data['response'] = batch_response\n",
    "    timing['time/get_response'] = time.time()-t\n",
    "    #print( game_data['response'])\n",
    "\n",
    "    #### tokenize text for sentiment analysis\n",
    "    t = time.time()\n",
    "    texts = [q + r for q,r in zip(query_list, game_data['response'])]\n",
    "    sentiment_inputs, attention_masks = build_bert_batch_from_txt(texts, sentiment_tokenizer, device)    \n",
    "    timing['time/build_input_sentiment_unused'] = time.time()-t\n",
    "        \n",
    "    #### get KG score\n",
    "    t = time.time()\n",
    "    pos_logits = []\n",
    "    for i in range(int(config['batch_size']/fbs)):\n",
    "        device = torch.device(\"cuda\")\n",
    "        # modify the rewards based on the KG\n",
    "        res = KG_check(i*fbs, (i+1)*fbs)\n",
    "        pos_logits.append(res)\n",
    "    \n",
    "    rewards = pos_logit_to_reward(torch.cat(pos_logits), task_list)\n",
    "    timing['time/get_factual_preds'] = time.time()-t\n",
    "\n",
    "\n",
    "    #### Run PPO training \n",
    "    t = time.time()\n",
    "    stats = ppo_trainer.step(query_tensors, response_tensors, rewards)\n",
    "    \n",
    "    # enforce nan to zero for wandb to run\n",
    "    stats = np.nan_to_num(stats)\n",
    "    # print(stats)\n",
    "    timing['time/optimization'] = time.time()-t\n",
    "     \n",
    "    #### Log everything\n",
    "    timing['time/epoch'] = time.time()-t0\n",
    "    table_rows = [list(r) for r in zip(game_data['query'], game_data['response'], rewards.cpu().tolist())]\n",
    "    logs.update({'game_log':wandb.Table(\n",
    "        columns=['query', 'response', 'reward'],\n",
    "        rows=table_rows)})\n",
    "    logs.update(timing)\n",
    "    # comment to remove the nan issues\n",
    "    logs.update(stats)\n",
    "    \n",
    "    # only support for float tensor\n",
    "    rewards = rewards.type(torch.FloatTensor)\n",
    "    \n",
    "    logs['env/reward_mean'] = torch.mean((rewards)).cpu().numpy()\n",
    "    logs['env/reward_std'] = torch.std(rewards).cpu().numpy()\n",
    "    logs['env/reward_dist'] = rewards.cpu().numpy()\n",
    "    for ctrl_s in ctrl_str:\n",
    "        key = 'env/reward_'+ctrl_s.strip('[]')\n",
    "        logs[key] = np.mean([r for r, t in zip(logs['env/reward_dist'], task_list) if t==ctrl_s])\n",
    "#     WandbCallback(log=None)\n",
    "    wandb.log(logs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "92999eb9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fact_eval(example_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "553e5560",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def pos_logit_to_reward(logit, task):\n",
    "    \"\"\"\n",
    "    Take the positive sentiment logit and scale it for the task.\n",
    "        task [negative]: reward = -logit\n",
    "        task [positive]: reward = logit\n",
    "    \"\"\"\n",
    "    for i in range(len(logit)):\n",
    "        if task[i]=='[false]':\n",
    "            logit[i] = -logit[i]\n",
    "        elif task[i]=='[true]':\n",
    "            pass\n",
    "        else:\n",
    "            raise ValueError('task has to be in [0, 1, 2]!')\n",
    "    return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "84f523ae",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### KG check to modify reward value \n",
    "\n",
    "# given a range i.e. i*fbs to (i+1)*fbs\n",
    "# should output a tensor([ 0, 0,  1, 1], device='cuda:0')\n",
    "\n",
    "# pred is the reponse in batch_response\n",
    "# query is the query_list\n",
    "\n",
    "def KG_check(lower, upper):\n",
    "    factuals = []\n",
    "    for i in range(lower,upper):\n",
    "        query = DBpedia.loc[i,\"query\"]\n",
    "        response = DBpedia.loc[i,\"response\"]\n",
    "        text = \"\" + query + response\n",
    "        # print(text)\n",
    "        factual = fact_eval(text)\n",
    "        factuals.append(factual)\n",
    "    # add variant \n",
    "    tensor_array = [i + random.random() for i in factuals ]\n",
    "    return torch.cuda.LongTensor(tensor_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3a4b589c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wz\\AppData\\Local\\Temp\\ipykernel_12672\\4003785611.py:20: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  return torch.cuda.LongTensor(tensor_array)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0], device='cuda:0')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KG_check(3, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1986dd29",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                          | 0/100 [00:00<?, ?it/s]C:\\Users\\wz\\AppData\\Local\\Temp\\ipykernel_12672\\4003785611.py:20: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  return torch.cuda.LongTensor(tensor_array)\n",
      " 14%|██████████▉                                                                   | 14/100 [40:00<4:30:20, 188.61s/it]\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m A graphql request initiated by the public wandb API timed out (timeout=9 sec). Create a new API with an integer timeout larger than 9, e.g., `api = wandb.Api(timeout=19)` to increase the graphql timeout.\n",
      " 19%|██████████████▊                                                               | 19/100 [59:00<4:26:55, 197.72s/it]\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m A graphql request initiated by the public wandb API timed out (timeout=9 sec). Create a new API with an integer timeout larger than 9, e.g., `api = wandb.Api(timeout=19)` to increase the graphql timeout.\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 100/100 [4:29:05<00:00, 161.46s/it]\n"
     ]
    }
   ],
   "source": [
    "# batch_size: 32\n",
    "# forward_batch_size: 4 \n",
    "\n",
    "ppo_trainer = PPOTrainer(gpt2_model, gpt2_model_ref, **config)\n",
    "fbs = config['forward_batch_size']\n",
    "\n",
    "for epoch in tqdm(range(int(np.ceil(config[\"steps\"]/config['batch_size'])))):\n",
    "    torch.cuda.empty_cache()\n",
    "    logs = dict()\n",
    "    game_data = dict()\n",
    "    timing = dict()\n",
    "    t0 = time.time()\n",
    "    \n",
    "    #### get a batch from the dataset and annotate tasks\n",
    "    df_batch = df.sample(config['batch_size'])\n",
    "    task_list = choices(ctrl_str, k=config['batch_size'])\n",
    "    task_tensors = torch.stack([ctrl_tokens[t] for t in task_list])\n",
    "    query_list = df_batch['query'].tolist()\n",
    "    game_data['query'] = [t+q for t,q in zip(task_list, query_list)]\n",
    "    \n",
    "    \n",
    "    #print(task_tensors)\n",
    "    query_tensors = torch.stack((df_batch['tokens'].tolist()))\n",
    "    #print(\"before \",query_tensors)\n",
    "    query_tensors = torch.cat((task_tensors, query_tensors), axis=1)\n",
    "\n",
    "    \n",
    "    #### get response from gpt2\n",
    "    t = time.time()\n",
    "    total_length = config['txt_in_len']+config['txt_out_len']\n",
    "    response_tensors_array = []\n",
    "    for i in range(int(config['batch_size']/fbs)):\n",
    "        response  = respond_to_batch(gpt2_model, query_tensors[i*fbs:(i+1)*fbs],\n",
    "                                     txt_len=config['txt_out_len'])\n",
    "        response_tensors_array.append(response)\n",
    "    response_tensors = torch.cat(response_tensors_array)\n",
    "#         #print(int(config['batch_size']/fbs))\n",
    "#         # 8 loops in total\n",
    "#         # 0-4, 5-8, 9-12...\n",
    "        \n",
    "#         # this is troublesome \n",
    "#         # error: probability tensor contains either `inf`, `nan` or element < 0\n",
    "#         #  next_token = torch.multinomial(probs, num_samples=1).squeeze(1) in gpt2.py function respond_to_batch\n",
    "        \n",
    "#         # response  = respond_to_batch(gpt2_model, query_tensors[i*fbs:(i+1)*fbs],txt_len=config['txt_out_len'])\n",
    "#         # we want a response of size [i*fbs:(i+1)*fbs]*1\n",
    "#         response_tensors = []\n",
    "#         for i in range(i*fbs, (i+1)*fbs):\n",
    "            \n",
    "#             seq = query_list[i]\n",
    "#             inpts = toker(seq, return_tensors=\"pt\")\n",
    "#             inpt_ids = inpts[\"input_ids\"]  # just IDS, no attn mask\n",
    "            \n",
    "#             key, loss = next_word_GPT2(i,100)\n",
    "            \n",
    "#             pred_id = base_tokenizer.encode(key, return_tensors = 'pt')\n",
    "#             # now it force response list into one tensor\n",
    "#             response = (pred_id[0])[1] # todo change it  should be one id not a list\n",
    "#             # print(response)\n",
    "#             response_tensors.append(response.item())\n",
    "#         response_tensors = torch.cuda.LongTensor(response_tensors)\n",
    "#         response_tensors_array.append(response_tensors)\n",
    "   \n",
    "#     # response_tensors.size = 32 * 1  \n",
    "#     response_tensors = torch.cat(response_tensors_array,axis = -1 )\n",
    "#     response_tensors  = torch.reshape(response_tensors, (-1, 1)) # force size of response_tensors.size = 32 * 1  rather that 32 length list\n",
    "#     # print(response_tensors)\n",
    "#     # print([gpt2_tokenizer.decode(response_tensors[i, :]) for i in range(config['batch_size'])])\n",
    "#     # print(response_tensors)\n",
    "    batch_response = [gpt2_tokenizer.decode(response_tensors[i, :]) for i in range(config['batch_size'])] # list of length 32\n",
    "    game_data['response'] = batch_response\n",
    "    timing['time/get_response'] = time.time()-t\n",
    "    #print( game_data['response'])\n",
    "\n",
    "    #### tokenize text for sentiment analysis\n",
    "    t = time.time()\n",
    "    texts = [q + r for q,r in zip(query_list, game_data['response'])]\n",
    "    sentiment_inputs, attention_masks = build_bert_batch_from_txt(texts, sentiment_tokenizer, device)    \n",
    "    timing['time/build_input_sentiment_unused'] = time.time()-t\n",
    "        \n",
    "    #### get KG score\n",
    "    t = time.time()\n",
    "    pos_logits = []\n",
    "    for i in range(int(config['batch_size']/fbs)):\n",
    "        device = torch.device(\"cuda\")\n",
    "        # modify the rewards based on the KG\n",
    "        res = KG_check(i*fbs, (i+1)*fbs)\n",
    "        pos_logits.append(res)\n",
    "    \n",
    "    rewards = pos_logit_to_reward(torch.cat(pos_logits), task_list)\n",
    "    timing['time/get_factual_preds'] = time.time()-t\n",
    "\n",
    "\n",
    "    #### Run PPO training \n",
    "    t = time.time()\n",
    "    stats = ppo_trainer.step(query_tensors, response_tensors, rewards)\n",
    "    \n",
    "    # enforce nan to zero for wandb to run\n",
    "    stats = np.nan_to_num(stats)\n",
    "    # print(stats)\n",
    "    timing['time/optimization'] = time.time()-t\n",
    "     \n",
    "    #### Log everything\n",
    "    timing['time/epoch'] = time.time()-t0\n",
    "    table_rows = [list(r) for r in zip(game_data['query'], game_data['response'], rewards.cpu().tolist())]\n",
    "    logs.update({'game_log':wandb.Table(\n",
    "        columns=['query', 'response', 'reward'],\n",
    "        rows=table_rows)})\n",
    "    logs.update(timing)\n",
    "    # comment to remove the nan issues\n",
    "    logs.update(stats)\n",
    "    \n",
    "    # only support for float tensor\n",
    "    rewards = rewards.type(torch.FloatTensor)\n",
    "    \n",
    "    logs['env/reward_mean'] = torch.mean((rewards)).cpu().numpy()\n",
    "    logs['env/reward_std'] = torch.std(rewards).cpu().numpy()\n",
    "    logs['env/reward_dist'] = rewards.cpu().numpy()\n",
    "    for ctrl_s in ctrl_str:\n",
    "        key = 'env/reward_'+ctrl_s.strip('[]')\n",
    "        logs[key] = np.mean([r for r, t in zip(logs['env/reward_dist'], task_list) if t==ctrl_s])\n",
    "#     WandbCallback(log=None)\n",
    "    wandb.log(logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147a8b0f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a92e4f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}